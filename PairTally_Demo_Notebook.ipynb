{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairTally Dataset Demonstration\n",
    "\n",
    "## Overview\n",
    "\n",
    "PairTally is the first benchmark specifically designed to evaluate fine-grained visual counting capabilities in computer vision models. This notebook provides a comprehensive demonstration of the dataset structure, visualization capabilities, and evaluation pipeline.\n",
    "\n",
    "### Paper\n",
    "\n",
    "**Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation**  \n",
    "Gia Khanh Nguyen, Yifeng Huang, Minh Hoai  \n",
    "Digital Image Computing: Techniques and Applications (DICTA) 2025\n",
    "\n",
    "### Dataset Specifications\n",
    "\n",
    "- **Total Images**: 681 high-resolution images\n",
    "- **Categories**: 54 object categories across 98 subcategories\n",
    "- **Task Types**: Inter-category (different objects) and Intra-category (same object, different attributes)\n",
    "- **Attribute Differences**: Color (43.5%), Shape/Texture (42.5%), Size (14.0%)\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Current state-of-the-art models achieve Mean Absolute Error (MAE) of 53.07, revealing critical gaps in fine-grained visual understanding and discrimination capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset paths\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATASET_DIR = BASE_DIR / \"dataset\" / \"pairtally_dataset\"\n",
    "ANNOTATIONS_DIR = DATASET_DIR / \"annotations\"\n",
    "IMAGES_DIR = DATASET_DIR / \"images\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify dataset structure and availability\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if not DATASET_DIR.exists():\n",
    "        issues.append(f\"Dataset directory not found: {DATASET_DIR}\")\n",
    "    \n",
    "    if not ANNOTATIONS_DIR.exists():\n",
    "        issues.append(f\"Annotations directory not found: {ANNOTATIONS_DIR}\")\n",
    "    \n",
    "    if not IMAGES_DIR.exists():\n",
    "        issues.append(f\"Images directory not found: {IMAGES_DIR}\")\n",
    "        issues.append(\"Download images from: https://drive.google.com/file/d/1TnenXS4yFicjo81NnmClfzgc8ltmmeBv/view\")\n",
    "    \n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            print(f\"ERROR: {issue}\")\n",
    "        return False\n",
    "    \n",
    "    # Count images\n",
    "    num_images = len(list(IMAGES_DIR.glob(\"*.jpg\")))\n",
    "    print(f\"Dataset verified successfully\")\n",
    "    print(f\"Found {num_images} images at {DATASET_DIR}\")\n",
    "    return True\n",
    "\n",
    "dataset_ready = verify_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTallyDataset:\n",
    "    \"\"\"PairTally dataset interface for loading and accessing annotations\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir: Path, version: str = \"simple\"):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.annotations_dir = dataset_dir / \"annotations\"\n",
    "        self.images_dir = dataset_dir / \"images\"\n",
    "        self.version = version  # \"simple\" or \"augmented\"\n",
    "        \n",
    "        # Load annotations\n",
    "        self.annotations = self._load_annotations()\n",
    "        self.inter_annotations = self._load_inter_annotations()\n",
    "        self.intra_annotations = self._load_intra_annotations()\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = self._load_metadata()\n",
    "        self.filename_mapping = self._load_filename_mapping()\n",
    "        \n",
    "        print(f\"Loaded {len(self.annotations)} total annotations\")\n",
    "        print(f\"  Inter-category: {len(self.inter_annotations)} images\")\n",
    "        print(f\"  Intra-category: {len(self.intra_annotations)} images\")\n",
    "    \n",
    "    def _load_json(self, filename: str) -> Dict:\n",
    "        \"\"\"Load a JSON file\"\"\"\n",
    "        filepath = self.annotations_dir / filename\n",
    "        if filepath.exists():\n",
    "            with open(filepath, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _load_annotations(self) -> Dict:\n",
    "        \"\"\"Load main annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_inter_annotations(self) -> Dict:\n",
    "        \"\"\"Load inter-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_inter_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_intra_annotations(self) -> Dict:\n",
    "        \"\"\"Load intra-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_intra_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_metadata(self) -> Dict:\n",
    "        \"\"\"Load image metadata\"\"\"\n",
    "        return self._load_json(\"image_metadata.json\")\n",
    "    \n",
    "    def _load_filename_mapping(self) -> Dict:\n",
    "        \"\"\"Load filename mapping\"\"\"\n",
    "        return self._load_json(\"filename_mapping.json\")\n",
    "    \n",
    "    def get_random_image(self, subset: str = \"all\") -> str:\n",
    "        \"\"\"Get a random image filename from the dataset\"\"\"\n",
    "        if subset == \"inter\":\n",
    "            images = list(self.inter_annotations.keys())\n",
    "        elif subset == \"intra\":\n",
    "            images = list(self.intra_annotations.keys())\n",
    "        else:\n",
    "            images = list(self.annotations.keys())\n",
    "        \n",
    "        return random.choice(images) if images else None\n",
    "    \n",
    "    def get_annotation(self, image_name: str) -> Dict:\n",
    "        \"\"\"Get annotation for a specific image\"\"\"\n",
    "        return self.annotations.get(image_name, {})\n",
    "    \n",
    "    def get_counts(self, image_name: str) -> Tuple[int, int]:\n",
    "        \"\"\"Get positive and negative class counts for an image\"\"\"\n",
    "        anno = self.get_annotation(image_name)\n",
    "        if anno:\n",
    "            positive_count = len(anno.get('points', []))\n",
    "            negative_count = len(anno.get('negative_points', []))\n",
    "            return positive_count, negative_count\n",
    "        return 0, 0\n",
    "    \n",
    "    def get_image_path(self, image_name: str) -> Path:\n",
    "        \"\"\"Get full path to an image\"\"\"\n",
    "        return self.images_dir / image_name\n",
    "\n",
    "# Initialize dataset\n",
    "if dataset_ready:\n",
    "    dataset = PairTallyDataset(DATASET_DIR, version=\"simple\")\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total images: {len(dataset.annotations)}\")\n",
    "    print(f\"Inter-category pairs: {len(dataset.inter_annotations)}\")\n",
    "    print(f\"Intra-category pairs: {len(dataset.intra_annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_with_annotations(dataset: PairTallyDataset, \n",
    "                                    image_name: str,\n",
    "                                    show_boxes: bool = True,\n",
    "                                    show_points: bool = False,\n",
    "                                    figsize: Tuple[int, int] = None):\n",
    "    \"\"\"\n",
    "    Visualize an image with bounding box annotations and object counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        image_name: Name of the image to visualize\n",
    "        show_boxes: Whether to show bounding box examples\n",
    "        show_points: Whether to show point annotations\n",
    "        figsize: Figure size for visualization\n",
    "    \"\"\"\n",
    "    # Get image path and annotation\n",
    "    img_path = dataset.get_image_path(image_name)\n",
    "    annotation = dataset.get_annotation(image_name)\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        print(f\"ERROR: Image not found: {img_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    # Auto-calculate figure size to maintain aspect ratio\n",
    "    if figsize is None:\n",
    "        aspect_ratio = width / height\n",
    "        fig_width = 10\n",
    "        fig_height = fig_width / aspect_ratio + 1\n",
    "        figsize = (fig_width, fig_height)\n",
    "    \n",
    "    # Get annotation data\n",
    "    positive_points = annotation.get('points', [])\n",
    "    negative_points = annotation.get('negative_points', [])\n",
    "    positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "    negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "    positive_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "    negative_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize, facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(img_array)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title with counts\n",
    "    blue_text = f\"{positive_prompt}: {len(positive_points)}\"\n",
    "    red_text = f\"{negative_prompt}: {len(negative_points)}\"\n",
    "    \n",
    "    fig.text(0.45, 0.95, blue_text, \n",
    "             ha='right', va='top', fontsize=14, color='#0066FF', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.5, 0.95, \" | \", \n",
    "             ha='center', va='top', fontsize=14, color='#666666', \n",
    "             fontweight='normal', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.55, 0.95, red_text, \n",
    "             ha='left', va='top', fontsize=14, color='#FF0040', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    # Plot bounding boxes\n",
    "    if show_boxes:\n",
    "        # Positive class boxes in blue\n",
    "        for box_coords in positive_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#0066FF', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        # Negative class boxes in red\n",
    "        for box_coords in negative_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#FF0040', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Plot points if requested\n",
    "    if show_points:\n",
    "        if positive_points:\n",
    "            pos_points = np.array(positive_points)\n",
    "            ax.scatter(pos_points[:, 0], pos_points[:, 1], \n",
    "                      c='#0066FF', s=8, alpha=0.4, marker='.')\n",
    "        \n",
    "        if negative_points:\n",
    "            neg_points = np.array(negative_points)\n",
    "            ax.scatter(neg_points[:, 0], neg_points[:, 1], \n",
    "                      c='#FF0040', s=8, alpha=0.4, marker='.')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.94])\n",
    "    plt.show()\n",
    "    \n",
    "    return positive_points, negative_points\n",
    "\n",
    "def display_dataset_statistics(dataset: PairTallyDataset):\n",
    "    \"\"\"Display comprehensive statistics about the PairTally dataset\"\"\"\n",
    "    \n",
    "    # Collect statistics\n",
    "    all_positive_counts = []\n",
    "    all_negative_counts = []\n",
    "    all_total_counts = []\n",
    "    \n",
    "    for img_name in dataset.annotations.keys():\n",
    "        pos_count, neg_count = dataset.get_counts(img_name)\n",
    "        all_positive_counts.append(pos_count)\n",
    "        all_negative_counts.append(neg_count)\n",
    "        all_total_counts.append(pos_count + neg_count)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Distribution of total counts\n",
    "    axes[0, 0].hist(all_total_counts, bins=30, color='#2E86AB', edgecolor='#1B4F72', alpha=0.8)\n",
    "    axes[0, 0].set_title('Distribution of Total Object Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Total Count')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(all_total_counts), color='#E74C3C', linestyle='--', linewidth=2,\n",
    "                      label=f'Mean: {np.mean(all_total_counts):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of positive vs negative counts\n",
    "    axes[0, 1].hist([all_positive_counts, all_negative_counts], \n",
    "                   bins=25, label=['Positive Class', 'Negative Class'],\n",
    "                   color=['#0066FF', '#FF0040'], alpha=0.7, edgecolor='#2C3E50')\n",
    "    axes[0, 1].set_title('Distribution of Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of positive vs negative counts\n",
    "    axes[1, 0].scatter(all_positive_counts, all_negative_counts, \n",
    "                      alpha=0.6, s=25, c='#8E44AD', edgecolors='#5B2C6F', linewidth=0.5)\n",
    "    axes[1, 0].set_title('Positive vs Negative Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Positive Class Count')\n",
    "    axes[1, 0].set_ylabel('Negative Class Count')\n",
    "    axes[1, 0].plot([0, max(all_positive_counts)], [0, max(all_positive_counts)], \n",
    "                   'k--', alpha=0.3, label='Equal counts', linewidth=1)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics\n",
    "    stats_text = f\"\"\"Dataset Summary Statistics\n",
    "    \n",
    "Total Images: {len(dataset.annotations)}\n",
    "Inter-category: {len(dataset.inter_annotations)}\n",
    "Intra-category: {len(dataset.intra_annotations)}\n",
    "\n",
    "Object Count Statistics:\n",
    "  Mean total: {np.mean(all_total_counts):.1f} ± {np.std(all_total_counts):.1f}\n",
    "  Min/Max: {min(all_total_counts)} / {max(all_total_counts)}\n",
    "  Median: {np.median(all_total_counts):.1f}\n",
    "\n",
    "Positive Class:\n",
    "  Mean: {np.mean(all_positive_counts):.1f} ± {np.std(all_positive_counts):.1f}\n",
    "  Range: [{min(all_positive_counts)}, {max(all_positive_counts)}]\n",
    "\n",
    "Negative Class:\n",
    "  Mean: {np.mean(all_negative_counts):.1f} ± {np.std(all_negative_counts):.1f}\n",
    "  Range: [{min(all_negative_counts)}, {max(all_negative_counts)}]\n",
    "\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='center', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.8', facecolor='#F8F9FA', alpha=0.9, edgecolor='#BDC3C7'))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('PairTally Dataset Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display statistics\n",
    "if dataset_ready:\n",
    "    print(\"Generating dataset statistics visualization...\")\n",
    "    display_dataset_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo: Select and Visualize Random Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_random_images(dataset: PairTallyDataset, \n",
    "                       num_images: int = 3,\n",
    "                       subset: str = \"all\"):\n",
    "    \"\"\"\n",
    "    Demonstrate dataset visualization with random image selection.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        num_images: Number of random images to display\n",
    "        subset: Which subset to use (\"all\", \"inter\", \"intra\")\n",
    "    \"\"\"\n",
    "    print(f\"\\nSelecting {num_images} random images from {subset} subset...\\n\")\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        # Get random image\n",
    "        image_name = dataset.get_random_image(subset=subset)\n",
    "        \n",
    "        if image_name:\n",
    "            # Get annotation info\n",
    "            annotation = dataset.get_annotation(image_name)\n",
    "            pos_count, neg_count = dataset.get_counts(image_name)\n",
    "            \n",
    "            # Get category names\n",
    "            pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "            neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "            \n",
    "            # Determine if inter or intra category\n",
    "            category_type = \"INTER\" if image_name in dataset.inter_annotations else \"INTRA\"\n",
    "            \n",
    "            print(f\"Image {i+1}/{num_images}: {image_name}\")\n",
    "            print(f\"  Category Type: {category_type}-category\")\n",
    "            print(f\"  Classes: {pos_prompt} vs {neg_prompt}\")\n",
    "            print(f\"  Counts: {pos_prompt}={pos_count}, {neg_prompt}={neg_count}, Total={pos_count+neg_count}\")\n",
    "            print()\n",
    "            \n",
    "            # Visualize with updated function\n",
    "            visualize_image_with_annotations(dataset, image_name, \n",
    "                                           show_boxes=True, show_points=False)\n",
    "            print(\"-\" * 80)\n",
    "\n",
    "def demo_specific_attribute_images(dataset: PairTallyDataset, \n",
    "                                  attribute_type: str = \"color\",\n",
    "                                  specific_image: str = None):\n",
    "    \"\"\"\n",
    "    Demonstrate specific attribute-based intra-category examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        attribute_type: Type of attribute difference (\"color\", \"size\", \"texture\")\n",
    "        specific_image: Specific image name to display (optional)\n",
    "    \"\"\"\n",
    "    if specific_image:\n",
    "        if specific_image in dataset.annotations:\n",
    "            annotation = dataset.get_annotation(specific_image)\n",
    "            pos_count, neg_count = dataset.get_counts(specific_image)\n",
    "            pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "            neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "            \n",
    "            print(f\"\\nSpecific Image: {specific_image}\")\n",
    "            print(f\"  Classes: {pos_prompt} vs {neg_prompt}\")\n",
    "            print(f\"  Counts: {pos_prompt}={pos_count}, {neg_prompt}={neg_count}\")\n",
    "            print()\n",
    "            \n",
    "            visualize_image_with_annotations(dataset, specific_image, \n",
    "                                           show_boxes=True, show_points=False)\n",
    "        else:\n",
    "            print(f\"ERROR: Image not found: {specific_image}\")\n",
    "    else:\n",
    "        # Find an intra-category image\n",
    "        intra_images = list(dataset.intra_annotations.keys())\n",
    "        if intra_images:\n",
    "            # Try to find images with specific patterns\n",
    "            candidates = []\n",
    "            \n",
    "            if attribute_type == \"color\":\n",
    "                keywords = ['COL', 'color', 'black', 'white', 'red', 'blue', 'green', 'yellow']\n",
    "            elif attribute_type == \"size\":\n",
    "                keywords = ['SIZ', 'size', 'big', 'small', 'large', 'tiny']\n",
    "            else:  # texture/shape\n",
    "                keywords = ['TEX', 'SHA', 'round', 'square', 'smooth', 'rough']\n",
    "            \n",
    "            # Filter images that might match\n",
    "            for img in intra_images:\n",
    "                if any(kw.lower() in img.lower() for kw in keywords):\n",
    "                    candidates.append(img)\n",
    "            \n",
    "            # If no specific matches, just use any intra image\n",
    "            if not candidates:\n",
    "                candidates = intra_images\n",
    "            \n",
    "            # Select and display\n",
    "            if candidates:\n",
    "                selected = random.choice(candidates)\n",
    "                annotation = dataset.get_annotation(selected)\n",
    "                pos_count, neg_count = dataset.get_counts(selected)\n",
    "                pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "                neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "                \n",
    "                print(f\"\\n{attribute_type.capitalize()} Difference Example: {selected}\")\n",
    "                print(f\"  Classes: {pos_prompt} vs {neg_prompt}\")\n",
    "                print(f\"  Counts: {pos_prompt}={pos_count}, {neg_prompt}={neg_count}\")\n",
    "                print()\n",
    "                \n",
    "                visualize_image_with_annotations(dataset, selected, \n",
    "                                               show_boxes=True, show_points=False)\n",
    "\n",
    "# Run comprehensive demonstration\n",
    "if dataset_ready:\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATASET VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Show 3 INTER-category examples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"1. INTER-CATEGORY EXAMPLES (Different Object Types)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=3, subset=\"inter\")\n",
    "    \n",
    "    # 2. Show 3 INTRA-category examples\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"2. INTRA-CATEGORY EXAMPLES (Same Object, Different Attributes)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=3, subset=\"intra\")\n",
    "    \n",
    "    # 3. Show specific attribute differences\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"3. SPECIFIC ATTRIBUTE DIFFERENCES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"\\nDemonstrating attribute-based discrimination challenges:\")\n",
    "    \n",
    "    # Color difference example\n",
    "    color_image = \"FOO_INTRA_CFC1_CFC2_077_077_33f3fb.jpg\"\n",
    "    print(f\"\\nCOLOR DIFFERENCE EXAMPLE:\")\n",
    "    demo_specific_attribute_images(dataset, specific_image=color_image)\n",
    "    \n",
    "    # Size difference example\n",
    "    size_image = \"HOU_INTRA_BAT1_BAT2_014_012_578d7d.jpg\"\n",
    "    print(f\"\\nSIZE DIFFERENCE EXAMPLE:\")\n",
    "    demo_specific_attribute_images(dataset, specific_image=size_image)\n",
    "    \n",
    "    # Texture/Shape difference example\n",
    "    texture_image = \"OTR_INTRA_NUT1_NUT2_030_055_f00b70.jpg\"\n",
    "    print(f\"\\nTEXTURE/SHAPE DIFFERENCE EXAMPLE:\")\n",
    "    demo_specific_attribute_images(dataset, specific_image=texture_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CountGD Model Evaluation Demo\n",
    "\n",
    "This section demonstrates how to use the CountGD model on PairTally dataset images. CountGD is a state-of-the-art object counting model that can use both exemplar boxes and text prompts for counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountGD model components\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Add CountGD to path\n",
    "countgd_path = BASE_DIR / \"models\" / \"countgd\" / \"CountGD\"\n",
    "sys.path.append(str(countgd_path))\n",
    "\n",
    "try:\n",
    "    from util.slconfig import SLConfig, DictAction\n",
    "    from util.misc import nested_tensor_from_tensor_list\n",
    "    import datasets_inference.transforms as T\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    \n",
    "    print(\"CountGD imports successful\")\n",
    "    countgd_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"CountGD import failed: {e}\")\n",
    "    print(\"Make sure CountGD dependencies are installed\")\n",
    "    countgd_available = False\n",
    "\n",
    "def load_countgd_model():\n",
    "    \"\"\"Load the CountGD model with default configuration\"\"\"\n",
    "    if not countgd_available:\n",
    "        print(\"CountGD not available - skipping model loading\")\n",
    "        return None, None\n",
    "        \n",
    "    # Set default paths\n",
    "    config_path = countgd_path / \"config\" / \"cfg_fsc147_vit_b.py\"\n",
    "    checkpoint_path = countgd_path / \"checkpoints\" / \"checkpoint_fsc147_best.pth\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(f\"Config file not found: {config_path}\")\n",
    "        print(\"Please ensure CountGD model files are properly set up\")\n",
    "        return None, None\n",
    "        \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        print(\"Please download the CountGD model checkpoint\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load config\n",
    "        cfg = SLConfig.fromfile(str(config_path))\n",
    "        cfg.merge_from_dict({\"text_encoder_type\": str(countgd_path / \"checkpoints\" / \"bert-base-uncased\")})\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Build model\n",
    "        build_func = MODULE_BUILD_FUNCS.get(\"groundingdino\")\n",
    "        if build_func is None:\n",
    "            print(\"Model builder not found\")\n",
    "            return None, None\n",
    "            \n",
    "        class Args:\n",
    "            def __init__(self):\n",
    "                self.modelname = \"groundingdino\"\n",
    "                self.device = str(device)\n",
    "        \n",
    "        args = Args()\n",
    "        for k, v in cfg._cfg_dict.items():\n",
    "            setattr(args, k, v)\n",
    "            \n",
    "        model, _, _ = build_func(args)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load checkpoint - FIX: Set weights_only=False for PyTorch 2.6+\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(str(checkpoint_path), map_location=device, weights_only=False)[\"model\"]\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create transform\n",
    "        transforms = T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(\"CountGD model loaded successfully!\")\n",
    "        return model, transforms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CountGD model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load the model\n",
    "if countgd_available:\n",
    "    print(\"Loading CountGD model...\")\n",
    "    countgd_model, countgd_transform = load_countgd_model()\n",
    "    model_ready = (countgd_model is not None)\n",
    "else:\n",
    "    model_ready = False\n",
    "    print(\"CountGD model not available for this demo\")\n",
    "\n",
    "print(f\"Model ready status: {model_ready}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_countgd_inference(model, transform, image, exemplar_boxes, text_prompt, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD inference on an image with exemplar boxes and text prompt\n",
    "    \n",
    "    Args:\n",
    "        model: CountGD model\n",
    "        transform: Image transform pipeline\n",
    "        image: PIL Image\n",
    "        exemplar_boxes: List of exemplar boxes in [x1, y1, x2, y2] format\n",
    "        text_prompt: Text description of objects to count\n",
    "        confidence_thresh: Confidence threshold for detections\n",
    "        \n",
    "    Returns:\n",
    "        pred_boxes: Predicted bounding boxes (normalized coordinates)\n",
    "        pred_logits: Prediction logits/scores\n",
    "        pred_count: Number of predicted objects\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Convert exemplar boxes to tensor format if available\n",
    "    if exemplar_boxes and len(exemplar_boxes) > 0:\n",
    "        # Convert to normalized coordinates relative to image size\n",
    "        img_width, img_height = image.size\n",
    "        exemplar_tensor = []\n",
    "        for box in exemplar_boxes[:3]:  # Limit to 3 exemplars\n",
    "            x1, y1, x2, y2 = box\n",
    "            # Normalize to [0, 1]\n",
    "            norm_x1 = x1 / img_width\n",
    "            norm_y1 = y1 / img_height\n",
    "            norm_x2 = x2 / img_width\n",
    "            norm_y2 = y2 / img_height\n",
    "            exemplar_tensor.append([norm_x1, norm_y1, norm_x2, norm_y2])\n",
    "        \n",
    "        exemplar_tensor = torch.tensor(exemplar_tensor, dtype=torch.float32)\n",
    "    else:\n",
    "        # Use empty tensor if no exemplars\n",
    "        exemplar_tensor = torch.tensor([], dtype=torch.float32).reshape(0, 4)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_image, target = transform(image, {\"exemplars\": exemplar_tensor})\n",
    "    input_image = input_image.to(device)\n",
    "    input_exemplar = target[\"exemplars\"].to(device)\n",
    "    \n",
    "    # Format text prompt\n",
    "    input_text = text_prompt + \" .\"\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        model_output = model(\n",
    "            input_image.unsqueeze(0),\n",
    "            [input_exemplar],\n",
    "            [torch.tensor([0]).to(device)],\n",
    "            captions=[input_text],\n",
    "        )\n",
    "    \n",
    "    # Extract predictions\n",
    "    logits = model_output[\"pred_logits\"][0].sigmoid()\n",
    "    boxes = model_output[\"pred_boxes\"][0]\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    box_mask = logits.max(dim=-1).values > confidence_thresh\n",
    "    filtered_logits = logits[box_mask, :]\n",
    "    filtered_boxes = boxes[box_mask, :]\n",
    "    pred_count = filtered_boxes.shape[0]\n",
    "    \n",
    "    return filtered_boxes, filtered_logits, pred_count\n",
    "\n",
    "def visualize_countgd_results(dataset, image_name, model, transform, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD on a PairTally image and visualize results comparing ground truth vs predictions.\n",
    "    \"\"\"\n",
    "    if not model_ready:\n",
    "        print(\"CountGD model not available\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Get image and annotation\n",
    "        img_path = dataset.get_image_path(image_name)\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"ERROR: Image not found: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Get annotation data\n",
    "        positive_points = annotation.get('points', [])\n",
    "        negative_points = annotation.get('negative_points', [])\n",
    "        positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "        negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "        positive_prompt = annotation.get('positive_prompt', 'objects')\n",
    "        negative_prompt = annotation.get('negative_prompt', 'objects')\n",
    "        \n",
    "        # Convert FSC147 format boxes to [x1, y1, x2, y2] format\n",
    "        def convert_boxes(box_coords_list):\n",
    "            boxes = []\n",
    "            for box_coords in box_coords_list[:3]:  # Use first 3 exemplars\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "            return boxes\n",
    "        \n",
    "        pos_exemplar_boxes = convert_boxes(positive_boxes)\n",
    "        neg_exemplar_boxes = convert_boxes(negative_boxes)\n",
    "        \n",
    "        # Get image dimensions for box conversion\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        print(f\"Running CountGD on: {image_name}\")\n",
    "        print(f\"Ground Truth - {positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}\")\n",
    "        \n",
    "        # Run inference for positive class\n",
    "        print(f\"\\nInferring positive class ({positive_prompt})...\")\n",
    "        pos_pred_boxes, pos_pred_logits, pos_pred_count = run_countgd_inference(\n",
    "            model, transform, image, pos_exemplar_boxes, positive_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        # Run inference for negative class  \n",
    "        print(f\"\\nInferring negative class ({negative_prompt})...\")\n",
    "        neg_pred_boxes, neg_pred_logits, neg_pred_count = run_countgd_inference(\n",
    "            model, transform, image, neg_exemplar_boxes, negative_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCountGD Predictions - {positive_prompt}: {pos_pred_count}, {negative_prompt}: {neg_pred_count}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f'CountGD Evaluation: {image_name}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Ground Truth with exemplars (no dots, just exemplar boxes)\n",
    "        axes[0].imshow(img_array)\n",
    "        axes[0].set_title(f'Ground Truth + Exemplars\\n{positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Draw exemplar boxes only (no ground truth points)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        # 2. Positive class predictions\n",
    "        axes[1].imshow(img_array)\n",
    "        axes[1].set_title(f'{positive_prompt} Predictions\\nGT: {len(positive_points)}, Pred: {pos_pred_count}', fontsize=10)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Draw positive exemplars (dashed)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[1].add_patch(rect)\n",
    "        \n",
    "        # Draw positive predictions (solid boxes)\n",
    "        if len(pos_pred_boxes) > 0 and pos_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            pos_pred_boxes_pixel = pos_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in pos_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width  \n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none')\n",
    "                    axes[1].add_patch(rect)\n",
    "        \n",
    "        # 3. Negative class predictions\n",
    "        axes[2].imshow(img_array)\n",
    "        axes[2].set_title(f'{negative_prompt} Predictions\\nGT: {len(negative_points)}, Pred: {neg_pred_count}', fontsize=10)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Draw negative exemplars (dashed)\n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[2].add_patch(rect)\n",
    "        \n",
    "        # Draw negative predictions (solid boxes)\n",
    "        if len(neg_pred_boxes) > 0 and neg_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            neg_pred_boxes_pixel = neg_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in neg_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width\n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none')\n",
    "                    axes[2].add_patch(rect)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate errors\n",
    "        pos_error = abs(pos_pred_count - len(positive_points))\n",
    "        neg_error = abs(neg_pred_count - len(negative_points))\n",
    "        total_error = pos_error + neg_error\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"  {positive_prompt} - GT: {len(positive_points)}, Pred: {pos_pred_count}, Error: {pos_error}\")\n",
    "        print(f\"  {negative_prompt} - GT: {len(negative_points)}, Pred: {neg_pred_count}, Error: {neg_error}\")\n",
    "        print(f\"  Total Error: {total_error}\")\n",
    "        print(f\"  Confidence Threshold: {confidence_thresh}\")\n",
    "        \n",
    "        return {\n",
    "            'positive_gt': len(positive_points),\n",
    "            'positive_pred': pos_pred_count,\n",
    "            'positive_error': pos_error,\n",
    "            'negative_gt': len(negative_points),\n",
    "            'negative_pred': neg_pred_count,\n",
    "            'negative_error': neg_error,\n",
    "            'total_error': total_error\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in CountGD evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"CountGD inference functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountGD Model Evaluation Demo\n",
    "if model_ready and dataset_ready:\n",
    "    print(\"=\"*80)\n",
    "    print(\"COUNTGD MODEL EVALUATION DEMONSTRATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Select a few example images to demonstrate CountGD\n",
    "    example_images = [\n",
    "        \"FOO_INTRA_CFC1_CFC2_077_077_33f3fb.jpg\",  # Color difference example\n",
    "        \"HOU_INTRA_BAT1_BAT2_014_012_578d7d.jpg\",  # Size difference example\n",
    "        \"OTR_INTRA_NUT1_NUT2_030_055_f00b70.jpg\",  # Texture/shape difference example\n",
    "    ]\n",
    "    \n",
    "    # Also include a random inter-category example\n",
    "    random_inter = dataset.get_random_image(subset=\"inter\")\n",
    "    if random_inter:\n",
    "        example_images.append(random_inter)\n",
    "    \n",
    "    print(f\"Running CountGD evaluation on {len(example_images)} example images...\")\n",
    "    print(\"This demonstrates how CountGD performs on PairTally's challenging fine-grained counting tasks.\\\\n\")\n",
    "    \n",
    "    evaluation_results = []\n",
    "    \n",
    "    for i, image_name in enumerate(example_images):\n",
    "        if image_name in dataset.annotations:\n",
    "            print(f\"\\\\n{'='*60}\")\n",
    "            print(f\"EXAMPLE {i+1}/{len(example_images)}: {image_name}\")\n",
    "            print('='*60)\n",
    "            \n",
    "            # Get image info\n",
    "            annotation = dataset.get_annotation(image_name)\n",
    "            pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "            neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "            category_type = \"INTER\" if image_name in dataset.inter_annotations else \"INTRA\"\n",
    "            \n",
    "            print(f\"Category Type: {category_type}-category\")\n",
    "            print(f\"Task: Count '{pos_prompt}' vs '{neg_prompt}'\")\n",
    "            print()\n",
    "            \n",
    "            # Run CountGD evaluation with visualization\n",
    "            try:\n",
    "                result = visualize_countgd_results(\n",
    "                    dataset, image_name, countgd_model, countgd_transform, \n",
    "                    confidence_thresh=0.3\n",
    "                )\n",
    "                \n",
    "                if result:\n",
    "                    evaluation_results.append({\n",
    "                        'image_name': image_name,\n",
    "                        'category_type': category_type,\n",
    "                        'positive_class': pos_prompt,\n",
    "                        'negative_class': neg_prompt,\n",
    "                        **result\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {image_name}: {e}\")\n",
    "                continue\n",
    "                \n",
    "        else:\n",
    "            print(f\"Skipping {image_name} - not found in annotations\")\n",
    "    \n",
    "    # Summary of results\n",
    "    if evaluation_results:\n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(\"COUNTGD EVALUATION SUMMARY\")\n",
    "        print('='*80)\n",
    "        \n",
    "        total_pos_error = sum(r['positive_error'] for r in evaluation_results)\n",
    "        total_neg_error = sum(r['negative_error'] for r in evaluation_results)\n",
    "        total_images = len(evaluation_results)\n",
    "        \n",
    "        avg_pos_error = total_pos_error / total_images\n",
    "        avg_neg_error = total_neg_error / total_images\n",
    "        avg_total_error = (total_pos_error + total_neg_error) / total_images\n",
    "        \n",
    "        print(f\"Results across {total_images} example images:\")\n",
    "        print(f\"  Average Positive Class MAE: {avg_pos_error:.2f}\")\n",
    "        print(f\"  Average Negative Class MAE: {avg_neg_error:.2f}\")\n",
    "        print(f\"  Average Total MAE: {avg_total_error:.2f}\")\n",
    "        print()\n",
    "        \n",
    "        print(\"Per-Image Results:\")\n",
    "        for result in evaluation_results:\n",
    "            print(f\"  {result['image_name'][:30]:<30} | {result['category_type']:<5} | \"\n",
    "                  f\"Total Error: {result['total_error']:2d} | \"\n",
    "                  f\"{result['positive_class']}: {result['positive_error']:2d} | \"\n",
    "                  f\"{result['negative_class']}: {result['negative_error']:2d}\")\n",
    "        \n",
    "        print(f\"\\\\n{'='*80}\")\n",
    "        print(\"KEY OBSERVATIONS:\")\n",
    "        print('='*80)\n",
    "        print(\"• CountGD uses exemplar boxes and text prompts for object counting\")\n",
    "        print(\"• The model processes each class (positive/negative) separately\")\n",
    "        print(\"• Performance varies significantly across different attribute types\")\n",
    "        print(\"• Fine-grained discrimination (e.g., color, size differences) remains challenging\")\n",
    "        print(\"• PairTally reveals limitations in current state-of-the-art counting models\")\n",
    "        \n",
    "else:\n",
    "    print(\"CountGD model evaluation demo not available\")\n",
    "    print(\"Reasons:\")\n",
    "    if not model_ready:\n",
    "        print(\"  - CountGD model not loaded (missing dependencies or checkpoints)\")\n",
    "    if not dataset_ready:\n",
    "        print(\"  - Dataset not ready (missing images or annotations)\")\n",
    "    print()\n",
    "    print(\"To enable this demo:\")\n",
    "    print(\"  1. Install CountGD dependencies\")\n",
    "    print(\"  2. Download CountGD model checkpoints\")\n",
    "    print(\"  3. Ensure PairTally dataset images are available\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "countgd_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
