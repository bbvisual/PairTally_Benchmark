{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairTally Dataset Demonstration\n",
    "\n",
    "## Overview\n",
    "\n",
    "PairTally is the first benchmark specifically designed to evaluate fine-grained visual counting capabilities in computer vision models. This notebook provides a comprehensive demonstration of the dataset structure, visualization capabilities, and evaluation pipeline.\n",
    "\n",
    "### Paper\n",
    "\n",
    "**Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation**  \n",
    "Gia Khanh Nguyen, Yifeng Huang, Minh Hoai  \n",
    "Digital Image Computing: Techniques and Applications (DICTA) 2025\n",
    "\n",
    "### Dataset Specifications\n",
    "\n",
    "- **Total Images**: 681 high-resolution images\n",
    "- **Categories**: 54 object categories across 98 subcategories\n",
    "- **Task Types**: Inter-category (different objects) and Intra-category (same object, different attributes)\n",
    "- **Attribute Differences**: Color (43.5%), Shape/Texture (42.5%), Size (14.0%)\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Current state-of-the-art models achieve Mean Absolute Error (MAE) of 53.07, revealing critical gaps in fine-grained visual understanding and discrimination capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset paths\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATASET_DIR = BASE_DIR / \"dataset\" / \"pairtally_dataset\"\n",
    "ANNOTATIONS_DIR = DATASET_DIR / \"annotations\"\n",
    "IMAGES_DIR = DATASET_DIR / \"images\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify dataset structure and availability\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if not DATASET_DIR.exists():\n",
    "        issues.append(f\"Dataset directory not found: {DATASET_DIR}\")\n",
    "    \n",
    "    if not ANNOTATIONS_DIR.exists():\n",
    "        issues.append(f\"Annotations directory not found: {ANNOTATIONS_DIR}\")\n",
    "    \n",
    "    if not IMAGES_DIR.exists():\n",
    "        issues.append(f\"Images directory not found: {IMAGES_DIR}\")\n",
    "        issues.append(\"Download images from: https://drive.google.com/file/d/1TnenXS4yFicjo81NnmClfzgc8ltmmeBv/view\")\n",
    "    \n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            print(f\"ERROR: {issue}\")\n",
    "        return False\n",
    "    \n",
    "    # Count images\n",
    "    num_images = len(list(IMAGES_DIR.glob(\"*.jpg\")))\n",
    "    print(f\"Dataset verified successfully\")\n",
    "    print(f\"Found {num_images} images at {DATASET_DIR}\")\n",
    "    return True\n",
    "\n",
    "dataset_ready = verify_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTallyDataset:\n",
    "    \"\"\"PairTally dataset interface for loading and accessing annotations\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir: Path, version: str = \"simple\"):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.annotations_dir = dataset_dir / \"annotations\"\n",
    "        self.images_dir = dataset_dir / \"images\"\n",
    "        self.version = version  # \"simple\" or \"augmented\"\n",
    "        \n",
    "        # Load annotations\n",
    "        self.annotations = self._load_annotations()\n",
    "        self.inter_annotations = self._load_inter_annotations()\n",
    "        self.intra_annotations = self._load_intra_annotations()\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = self._load_metadata()\n",
    "        self.filename_mapping = self._load_filename_mapping()\n",
    "        \n",
    "        print(f\"Loaded {len(self.annotations)} total annotations\")\n",
    "        print(f\"  Inter-category: {len(self.inter_annotations)} images\")\n",
    "        print(f\"  Intra-category: {len(self.intra_annotations)} images\")\n",
    "    \n",
    "    def _load_json(self, filename: str) -> Dict:\n",
    "        \"\"\"Load a JSON file\"\"\"\n",
    "        filepath = self.annotations_dir / filename\n",
    "        if filepath.exists():\n",
    "            with open(filepath, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _load_annotations(self) -> Dict:\n",
    "        \"\"\"Load main annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_inter_annotations(self) -> Dict:\n",
    "        \"\"\"Load inter-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_inter_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_intra_annotations(self) -> Dict:\n",
    "        \"\"\"Load intra-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_intra_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_metadata(self) -> Dict:\n",
    "        \"\"\"Load image metadata\"\"\"\n",
    "        return self._load_json(\"image_metadata.json\")\n",
    "    \n",
    "    def _load_filename_mapping(self) -> Dict:\n",
    "        \"\"\"Load filename mapping\"\"\"\n",
    "        return self._load_json(\"filename_mapping.json\")\n",
    "    \n",
    "    def get_random_image(self, subset: str = \"all\") -> str:\n",
    "        \"\"\"Get a random image filename from the dataset\"\"\"\n",
    "        if subset == \"inter\":\n",
    "            images = list(self.inter_annotations.keys())\n",
    "        elif subset == \"intra\":\n",
    "            images = list(self.intra_annotations.keys())\n",
    "        else:\n",
    "            images = list(self.annotations.keys())\n",
    "        \n",
    "        return random.choice(images) if images else None\n",
    "    \n",
    "    def get_annotation(self, image_name: str) -> Dict:\n",
    "        \"\"\"Get annotation for a specific image\"\"\"\n",
    "        return self.annotations.get(image_name, {})\n",
    "    \n",
    "    def get_counts(self, image_name: str) -> Tuple[int, int]:\n",
    "        \"\"\"Get positive and negative class counts for an image\"\"\"\n",
    "        anno = self.get_annotation(image_name)\n",
    "        if anno:\n",
    "            positive_count = len(anno.get('points', []))\n",
    "            negative_count = len(anno.get('negative_points', []))\n",
    "            return positive_count, negative_count\n",
    "        return 0, 0\n",
    "    \n",
    "    def get_image_path(self, image_name: str) -> Path:\n",
    "        \"\"\"Get full path to an image\"\"\"\n",
    "        return self.images_dir / image_name\n",
    "\n",
    "# Initialize dataset\n",
    "if dataset_ready:\n",
    "    dataset = PairTallyDataset(DATASET_DIR, version=\"simple\")\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total images: {len(dataset.annotations)}\")\n",
    "    print(f\"Inter-category pairs: {len(dataset.inter_annotations)}\")\n",
    "    print(f\"Intra-category pairs: {len(dataset.intra_annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_with_annotations(dataset: PairTallyDataset, \n",
    "                                    image_name: str,\n",
    "                                    show_boxes: bool = True,\n",
    "                                    show_points: bool = False,\n",
    "                                    figsize: Tuple[int, int] = None):\n",
    "    \"\"\"\n",
    "    Visualize an image with bounding box annotations and object counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        image_name: Name of the image to visualize\n",
    "        show_boxes: Whether to show bounding box examples\n",
    "        show_points: Whether to show point annotations\n",
    "        figsize: Figure size for visualization\n",
    "    \"\"\"\n",
    "    # Get image path and annotation\n",
    "    img_path = dataset.get_image_path(image_name)\n",
    "    annotation = dataset.get_annotation(image_name)\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        print(f\"ERROR: Image not found: {img_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    # Auto-calculate figure size to maintain aspect ratio\n",
    "    if figsize is None:\n",
    "        aspect_ratio = width / height\n",
    "        fig_width = 10\n",
    "        fig_height = fig_width / aspect_ratio + 1\n",
    "        figsize = (fig_width, fig_height)\n",
    "    \n",
    "    # Get annotation data\n",
    "    positive_points = annotation.get('points', [])\n",
    "    negative_points = annotation.get('negative_points', [])\n",
    "    positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "    negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "    positive_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "    negative_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize, facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(img_array)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title with counts\n",
    "    blue_text = f\"{positive_prompt}: {len(positive_points)}\"\n",
    "    red_text = f\"{negative_prompt}: {len(negative_points)}\"\n",
    "    \n",
    "    fig.text(0.45, 0.95, blue_text, \n",
    "             ha='right', va='top', fontsize=14, color='#0066FF', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.5, 0.95, \" | \", \n",
    "             ha='center', va='top', fontsize=14, color='#666666', \n",
    "             fontweight='normal', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.55, 0.95, red_text, \n",
    "             ha='left', va='top', fontsize=14, color='#FF0040', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    # Plot bounding boxes\n",
    "    if show_boxes:\n",
    "        # Positive class boxes in blue\n",
    "        for box_coords in positive_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#0066FF', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        # Negative class boxes in red\n",
    "        for box_coords in negative_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#FF0040', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Plot points if requested\n",
    "    if show_points:\n",
    "        if positive_points:\n",
    "            pos_points = np.array(positive_points)\n",
    "            ax.scatter(pos_points[:, 0], pos_points[:, 1], \n",
    "                      c='#0066FF', s=8, alpha=0.4, marker='.')\n",
    "        \n",
    "        if negative_points:\n",
    "            neg_points = np.array(negative_points)\n",
    "            ax.scatter(neg_points[:, 0], neg_points[:, 1], \n",
    "                      c='#FF0040', s=8, alpha=0.4, marker='.')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.94])\n",
    "    plt.show()\n",
    "    \n",
    "    return positive_points, negative_points\n",
    "\n",
    "def visualize_three_images(dataset: PairTallyDataset, \n",
    "                          image_names: List[str],\n",
    "                          show_boxes: bool = True,\n",
    "                          show_points: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize exactly 3 images in a horizontal layout.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        image_names: List of exactly 3 image names to visualize\n",
    "        show_boxes: Whether to show bounding box examples\n",
    "        show_points: Whether to show point annotations\n",
    "    \"\"\"\n",
    "    if len(image_names) != 3:\n",
    "        print(f\"ERROR: This function requires exactly 3 images, got {len(image_names)}\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with 3 horizontal subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='white')\n",
    "    \n",
    "    for i, image_name in enumerate(image_names):\n",
    "        # Get image path and annotation\n",
    "        img_path = dataset.get_image_path(image_name)\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"ERROR: Image not found: {img_path}\")\n",
    "            axes[i].text(0.5, 0.5, f\"Image not found:\\n{image_name}\", \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Get annotation data\n",
    "        positive_points = annotation.get('points', [])\n",
    "        negative_points = annotation.get('negative_points', [])\n",
    "        positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "        negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "        positive_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        negative_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(img_array)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add title with colored text\n",
    "        blue_text = f\"{positive_prompt}: {len(positive_points)}\"\n",
    "        red_text = f\"{negative_prompt}: {len(negative_points)}\"\n",
    "        \n",
    "        # Position title above each subplot\n",
    "        fig.text(0.17 + i*0.33, 0.92, blue_text, \n",
    "                ha='center', va='top', fontsize=12, color='#0066FF', \n",
    "                fontweight='bold', transform=fig.transFigure)\n",
    "        \n",
    "        fig.text(0.17 + i*0.33, 0.88, red_text, \n",
    "                ha='center', va='top', fontsize=12, color='#FF0040', \n",
    "                fontweight='bold', transform=fig.transFigure)\n",
    "        \n",
    "        # Add image filename at bottom\n",
    "        axes[i].text(0.5, -0.05, image_name, \n",
    "                    ha='center', va='top', fontsize=10, \n",
    "                    transform=axes[i].transAxes, weight='bold')\n",
    "        \n",
    "        # Plot bounding boxes\n",
    "        if show_boxes:\n",
    "            # Positive class boxes in blue\n",
    "            for box_coords in positive_boxes[:3]:\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor='#0066FF', \n",
    "                                   facecolor='none')\n",
    "                    axes[i].add_patch(rect)\n",
    "            \n",
    "            # Negative class boxes in red\n",
    "            for box_coords in negative_boxes[:3]:\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor='#FF0040', \n",
    "                                   facecolor='none')\n",
    "                    axes[i].add_patch(rect)\n",
    "        \n",
    "        # Plot points if requested\n",
    "        if show_points:\n",
    "            if positive_points:\n",
    "                pos_points = np.array(positive_points)\n",
    "                axes[i].scatter(pos_points[:, 0], pos_points[:, 1], \n",
    "                              c='#0066FF', s=8, alpha=0.4, marker='.')\n",
    "            \n",
    "            if negative_points:\n",
    "                neg_points = np.array(negative_points)\n",
    "                axes[i].scatter(neg_points[:, 0], neg_points[:, 1], \n",
    "                              c='#FF0040', s=8, alpha=0.4, marker='.')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.85])\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_statistics(dataset: PairTallyDataset):\n",
    "    \"\"\"Display comprehensive statistics about the PairTally dataset\"\"\"\n",
    "    \n",
    "    # Collect statistics\n",
    "    all_positive_counts = []\n",
    "    all_negative_counts = []\n",
    "    all_total_counts = []\n",
    "    \n",
    "    for img_name in dataset.annotations.keys():\n",
    "        pos_count, neg_count = dataset.get_counts(img_name)\n",
    "        all_positive_counts.append(pos_count)\n",
    "        all_negative_counts.append(neg_count)\n",
    "        all_total_counts.append(pos_count + neg_count)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Distribution of total counts\n",
    "    axes[0, 0].hist(all_total_counts, bins=30, color='#2E86AB', edgecolor='#1B4F72', alpha=0.8)\n",
    "    axes[0, 0].set_title('Distribution of Total Object Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Total Count')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(all_total_counts), color='#E74C3C', linestyle='--', linewidth=2,\n",
    "                      label=f'Mean: {np.mean(all_total_counts):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of positive vs negative counts\n",
    "    axes[0, 1].hist([all_positive_counts, all_negative_counts], \n",
    "                   bins=25, label=['Positive Class', 'Negative Class'],\n",
    "                   color=['#0066FF', '#FF0040'], alpha=0.7, edgecolor='#2C3E50')\n",
    "    axes[0, 1].set_title('Distribution of Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of positive vs negative counts\n",
    "    axes[1, 0].scatter(all_positive_counts, all_negative_counts, \n",
    "                      alpha=0.6, s=25, c='#8E44AD', edgecolors='#5B2C6F', linewidth=0.5)\n",
    "    axes[1, 0].set_title('Positive vs Negative Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Positive Class Count')\n",
    "    axes[1, 0].set_ylabel('Negative Class Count')\n",
    "    axes[1, 0].plot([0, max(all_positive_counts)], [0, max(all_positive_counts)], \n",
    "                   'k--', alpha=0.3, label='Equal counts', linewidth=1)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics\n",
    "    stats_text = f\"\"\"Dataset Summary Statistics\n",
    "    \n",
    "Total Images: {len(dataset.annotations)}\n",
    "Inter-category: {len(dataset.inter_annotations)}\n",
    "Intra-category: {len(dataset.intra_annotations)}\n",
    "\n",
    "Object Count Statistics:\n",
    "  Mean total: {np.mean(all_total_counts):.1f} ± {np.std(all_total_counts):.1f}\n",
    "  Min/Max: {min(all_total_counts)} / {max(all_total_counts)}\n",
    "  Median: {np.median(all_total_counts):.1f}\n",
    "\n",
    "Positive Class:\n",
    "  Mean: {np.mean(all_positive_counts):.1f} ± {np.std(all_positive_counts):.1f}\n",
    "  Range: [{min(all_positive_counts)}, {max(all_positive_counts)}]\n",
    "\n",
    "Negative Class:\n",
    "  Mean: {np.mean(all_negative_counts):.1f} ± {np.std(all_negative_counts):.1f}\n",
    "  Range: [{min(all_negative_counts)}, {max(all_negative_counts)}]\n",
    "\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='center', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.8', facecolor='#F8F9FA', alpha=0.9, edgecolor='#BDC3C7'))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('PairTally Dataset Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display statistics\n",
    "if dataset_ready:\n",
    "    print(\"Generating dataset statistics visualization...\")\n",
    "    display_dataset_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo: Select and Visualize Random Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_random_images(dataset: PairTallyDataset, \n",
    "                       num_images: int = 6,\n",
    "                       subset: str = \"all\"):\n",
    "    \"\"\"\n",
    "    Demonstrate dataset visualization with random image selection.\n",
    "    Show images in groups of 3 to reduce scrolling.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        num_images: Number of random images to display\n",
    "        subset: Which subset to use (\"all\", \"inter\", \"intra\")\n",
    "    \"\"\"\n",
    "    print(f\"\\nSelecting {num_images} random images from {subset} subset...\\n\")\n",
    "    \n",
    "    # Get random images\n",
    "    selected_images = []\n",
    "    for i in range(num_images):\n",
    "        image_name = dataset.get_random_image(subset=subset)\n",
    "        if image_name and image_name not in selected_images:\n",
    "            selected_images.append(image_name)\n",
    "        elif image_name in selected_images:\n",
    "            # Try again with different random seed\n",
    "            for _ in range(10):  # Max 10 retries\n",
    "                alt_image = dataset.get_random_image(subset=subset)\n",
    "                if alt_image and alt_image not in selected_images:\n",
    "                    selected_images.append(alt_image)\n",
    "                    break\n",
    "    \n",
    "    if not selected_images:\n",
    "        print(\"No images found\")\n",
    "        return\n",
    "    \n",
    "    # Display info about selected images\n",
    "    for i, image_name in enumerate(selected_images):\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        pos_count, neg_count = dataset.get_counts(image_name)\n",
    "        pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        category_type = \"INTER\" if image_name in dataset.inter_annotations else \"INTRA\"\n",
    "        \n",
    "        print(f\"Image {i+1}: {image_name[:40]}\")\n",
    "        print(f\"  Type: {category_type} | Classes: {pos_prompt} vs {neg_prompt} | Counts: {pos_count}+{neg_count}={pos_count+neg_count}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Display images in groups of 3\n",
    "    for i in range(0, len(selected_images), 3):\n",
    "        group = selected_images[i:i+3]\n",
    "        \n",
    "        if len(group) == 3:\n",
    "            print(f\"\\\\nGroup {i//3 + 1}: Images {i+1}-{i+3}\")\n",
    "            visualize_three_images(dataset, group, show_boxes=True, show_points=False)\n",
    "        else:\n",
    "            # For remaining images that don't make a group of 3\n",
    "            print(f\"\\\\nRemaining images:\")\n",
    "            for img_name in group:\n",
    "                visualize_image_with_annotations(dataset, img_name, \n",
    "                                               show_boxes=True, show_points=False)\n",
    "\n",
    "def demo_specific_attribute_images(dataset: PairTallyDataset, \n",
    "                                  attribute_type: str = \"color\",\n",
    "                                  num_examples: int = 3):\n",
    "    \"\"\"\n",
    "    Demonstrate specific attribute-based intra-category examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        attribute_type: Type of attribute difference (\"color\", \"size\", \"texture\")\n",
    "        num_examples: Number of examples to show (will show exactly 3)\n",
    "    \"\"\"\n",
    "    # Find intra-category images with specific patterns\n",
    "    intra_images = list(dataset.intra_annotations.keys())\n",
    "    candidates = []\n",
    "    \n",
    "    if attribute_type == \"color\":\n",
    "        keywords = ['COL', 'color', 'black', 'white', 'red', 'blue', 'green', 'yellow']\n",
    "    elif attribute_type == \"size\":\n",
    "        keywords = ['SIZ', 'size', 'big', 'small', 'large', 'tiny']\n",
    "    else:  # texture/shape\n",
    "        keywords = ['TEX', 'SHA', 'round', 'square', 'smooth', 'rough']\n",
    "    \n",
    "    # Filter images that might match\n",
    "    for img in intra_images:\n",
    "        if any(kw.lower() in img.lower() for kw in keywords):\n",
    "            candidates.append(img)\n",
    "    \n",
    "    # If no specific matches, just use any intra images\n",
    "    if not candidates:\n",
    "        candidates = intra_images\n",
    "    \n",
    "    # Select exactly 3 examples\n",
    "    selected = random.sample(candidates, min(3, len(candidates)))\n",
    "    \n",
    "    print(f\"\\\\n{attribute_type.upper()} DIFFERENCE EXAMPLES:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display info\n",
    "    for i, image_name in enumerate(selected):\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        pos_count, neg_count = dataset.get_counts(image_name)\n",
    "        pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        \n",
    "        print(f\"Example {i+1}: {image_name[:40]}\")\n",
    "        print(f\"  Classes: {pos_prompt} vs {neg_prompt} | Counts: {pos_count}+{neg_count}={pos_count+neg_count}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Visualize as a group of 3\n",
    "    if len(selected) == 3:\n",
    "        visualize_three_images(dataset, selected, show_boxes=True, show_points=False)\n",
    "    else:\n",
    "        # Fallback for less than 3 images\n",
    "        for image_name in selected:\n",
    "            visualize_image_with_annotations(dataset, image_name, \n",
    "                                           show_boxes=True, show_points=False)\n",
    "\n",
    "# Run comprehensive demonstration\n",
    "if dataset_ready:\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATASET VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Show 6 INTER-category examples (2 groups of 3)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"1. INTER-CATEGORY EXAMPLES (Different Object Types)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=6, subset=\"inter\")\n",
    "    \n",
    "    # 2. Show 6 INTRA-category examples (2 groups of 3)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"2. INTRA-CATEGORY EXAMPLES (Same Object, Different Attributes)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=6, subset=\"intra\")\n",
    "    \n",
    "    # 3. Show specific attribute differences (3 examples each)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"3. SPECIFIC ATTRIBUTE DIFFERENCES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Demonstrating attribute-based discrimination challenges:\")\n",
    "    \n",
    "    # Color difference examples\n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"color\", \n",
    "                                 num_examples=3)\n",
    "    \n",
    "    # Size difference examples  \n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"size\", \n",
    "                                 num_examples=3)\n",
    "    \n",
    "    # Texture/Shape difference examples\n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"texture\", \n",
    "                                 num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CountGD Model Evaluation Demo\n",
    "\n",
    "This section demonstrates how to use the CountGD model on PairTally dataset images. CountGD is a state-of-the-art object counting model that can use both exemplar boxes and text prompts for counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountGD model components\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Add CountGD to path\n",
    "countgd_path = BASE_DIR / \"models\" / \"countgd\" / \"CountGD\"\n",
    "sys.path.append(str(countgd_path))\n",
    "\n",
    "try:\n",
    "    from util.slconfig import SLConfig, DictAction\n",
    "    from util.misc import nested_tensor_from_tensor_list\n",
    "    import datasets_inference.transforms as T\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    \n",
    "    print(\"CountGD imports successful\")\n",
    "    countgd_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"CountGD import failed: {e}\")\n",
    "    print(\"Make sure CountGD dependencies are installed\")\n",
    "    countgd_available = False\n",
    "\n",
    "def load_countgd_model():\n",
    "    \"\"\"Load the CountGD model with default configuration\"\"\"\n",
    "    if not countgd_available:\n",
    "        print(\"CountGD not available - skipping model loading\")\n",
    "        return None, None\n",
    "        \n",
    "    # Set default paths\n",
    "    config_path = countgd_path / \"config\" / \"cfg_fsc147_vit_b.py\"\n",
    "    checkpoint_path = countgd_path / \"checkpoints\" / \"checkpoint_fsc147_best.pth\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(f\"Config file not found: {config_path}\")\n",
    "        print(\"Please ensure CountGD model files are properly set up\")\n",
    "        return None, None\n",
    "        \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        print(\"Please download the CountGD model checkpoint\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load config\n",
    "        cfg = SLConfig.fromfile(str(config_path))\n",
    "        cfg.merge_from_dict({\"text_encoder_type\": str(countgd_path / \"checkpoints\" / \"bert-base-uncased\")})\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Build model\n",
    "        build_func = MODULE_BUILD_FUNCS.get(\"groundingdino\")\n",
    "        if build_func is None:\n",
    "            print(\"Model builder not found\")\n",
    "            return None, None\n",
    "            \n",
    "        class Args:\n",
    "            def __init__(self):\n",
    "                self.modelname = \"groundingdino\"\n",
    "                self.device = str(device)\n",
    "        \n",
    "        args = Args()\n",
    "        for k, v in cfg._cfg_dict.items():\n",
    "            setattr(args, k, v)\n",
    "            \n",
    "        model, _, _ = build_func(args)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load checkpoint - FIX: Set weights_only=False for PyTorch 2.6+\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(str(checkpoint_path), map_location=device, weights_only=False)[\"model\"]\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create transform\n",
    "        transforms = T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(\"CountGD model loaded successfully!\")\n",
    "        return model, transforms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CountGD model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load the model\n",
    "if countgd_available:\n",
    "    print(\"Loading CountGD model...\")\n",
    "    countgd_model, countgd_transform = load_countgd_model()\n",
    "    model_ready = (countgd_model is not None)\n",
    "else:\n",
    "    model_ready = False\n",
    "    print(\"CountGD model not available for this demo\")\n",
    "\n",
    "print(f\"Model ready status: {model_ready}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_countgd_inference(model, transform, image, exemplar_boxes, text_prompt, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD inference on an image with exemplar boxes and text prompt\n",
    "    \n",
    "    Args:\n",
    "        model: CountGD model\n",
    "        transform: Image transform pipeline\n",
    "        image: PIL Image\n",
    "        exemplar_boxes: List of exemplar boxes in [x1, y1, x2, y2] format\n",
    "        text_prompt: Text description of objects to count\n",
    "        confidence_thresh: Confidence threshold for detections\n",
    "        \n",
    "    Returns:\n",
    "        pred_boxes: Predicted bounding boxes (normalized coordinates)\n",
    "        pred_logits: Prediction logits/scores\n",
    "        pred_count: Number of predicted objects\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Convert exemplar boxes to tensor format if available\n",
    "    if exemplar_boxes and len(exemplar_boxes) > 0:\n",
    "        # Convert to normalized coordinates relative to image size\n",
    "        img_width, img_height = image.size\n",
    "        exemplar_tensor = []\n",
    "        for box in exemplar_boxes[:3]:  # Limit to 3 exemplars\n",
    "            x1, y1, x2, y2 = box\n",
    "            # Normalize to [0, 1]\n",
    "            norm_x1 = x1 / img_width\n",
    "            norm_y1 = y1 / img_height\n",
    "            norm_x2 = x2 / img_width\n",
    "            norm_y2 = y2 / img_height\n",
    "            exemplar_tensor.append([norm_x1, norm_y1, norm_x2, norm_y2])\n",
    "        \n",
    "        exemplar_tensor = torch.tensor(exemplar_tensor, dtype=torch.float32)\n",
    "    else:\n",
    "        # Use empty tensor if no exemplars\n",
    "        exemplar_tensor = torch.tensor([], dtype=torch.float32).reshape(0, 4)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_image, target = transform(image, {\"exemplars\": exemplar_tensor})\n",
    "    input_image = input_image.to(device)\n",
    "    input_exemplar = target[\"exemplars\"].to(device)\n",
    "    \n",
    "    # Format text prompt\n",
    "    input_text = text_prompt + \" .\"\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        model_output = model(\n",
    "            input_image.unsqueeze(0),\n",
    "            [input_exemplar],\n",
    "            [torch.tensor([0]).to(device)],\n",
    "            captions=[input_text],\n",
    "        )\n",
    "    \n",
    "    # Extract predictions\n",
    "    logits = model_output[\"pred_logits\"][0].sigmoid()\n",
    "    boxes = model_output[\"pred_boxes\"][0]\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    box_mask = logits.max(dim=-1).values > confidence_thresh\n",
    "    filtered_logits = logits[box_mask, :]\n",
    "    filtered_boxes = boxes[box_mask, :]\n",
    "    pred_count = filtered_boxes.shape[0]\n",
    "    \n",
    "    return filtered_boxes, filtered_logits, pred_count\n",
    "\n",
    "def visualize_countgd_results(dataset, image_name, model, transform, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD on a PairTally image and visualize results comparing ground truth vs predictions.\n",
    "    \"\"\"\n",
    "    if not model_ready:\n",
    "        print(\"CountGD model not available\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Get image and annotation\n",
    "        img_path = dataset.get_image_path(image_name)\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"ERROR: Image not found: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Get annotation data\n",
    "        positive_points = annotation.get('points', [])\n",
    "        negative_points = annotation.get('negative_points', [])\n",
    "        positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "        negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "        positive_prompt = annotation.get('positive_prompt', 'objects')\n",
    "        negative_prompt = annotation.get('negative_prompt', 'objects')\n",
    "        \n",
    "        # Convert FSC147 format boxes to [x1, y1, x2, y2] format\n",
    "        def convert_boxes(box_coords_list):\n",
    "            boxes = []\n",
    "            for box_coords in box_coords_list[:3]:  # Use first 3 exemplars\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "            return boxes\n",
    "        \n",
    "        pos_exemplar_boxes = convert_boxes(positive_boxes)\n",
    "        neg_exemplar_boxes = convert_boxes(negative_boxes)\n",
    "        \n",
    "        # Get image dimensions for box conversion\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        print(f\"Running CountGD on: {image_name}\")\n",
    "        print(f\"Ground Truth - {positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}\")\n",
    "        \n",
    "        # Run inference for positive class\n",
    "        print(f\"\\nInferring positive class ({positive_prompt})...\")\n",
    "        pos_pred_boxes, pos_pred_logits, pos_pred_count = run_countgd_inference(\n",
    "            model, transform, image, pos_exemplar_boxes, positive_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        # Run inference for negative class  \n",
    "        print(f\"\\nInferring negative class ({negative_prompt})...\")\n",
    "        neg_pred_boxes, neg_pred_logits, neg_pred_count = run_countgd_inference(\n",
    "            model, transform, image, neg_exemplar_boxes, negative_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCountGD Predictions - {positive_prompt}: {pos_pred_count}, {negative_prompt}: {neg_pred_count}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f'CountGD Evaluation: {image_name}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Ground Truth with exemplars (no dots, just exemplar boxes)\n",
    "        axes[0].imshow(img_array)\n",
    "        axes[0].set_title(f'Ground Truth + Exemplars\\n{positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Draw exemplar boxes only (no ground truth points)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        # 2. Positive class predictions\n",
    "        axes[1].imshow(img_array)\n",
    "        axes[1].set_title(f'{positive_prompt} Predictions\\nGT: {len(positive_points)}, Pred: {pos_pred_count}', fontsize=10)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Draw positive exemplars (dashed)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[1].add_patch(rect)\n",
    "        \n",
    "        # Draw positive predictions (solid boxes)\n",
    "        if len(pos_pred_boxes) > 0 and pos_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            pos_pred_boxes_pixel = pos_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in pos_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width  \n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none')\n",
    "                    axes[1].add_patch(rect)\n",
    "        \n",
    "        # 3. Negative class predictions\n",
    "        axes[2].imshow(img_array)\n",
    "        axes[2].set_title(f'{negative_prompt} Predictions\\nGT: {len(negative_points)}, Pred: {neg_pred_count}', fontsize=10)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Draw negative exemplars (dashed)\n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[2].add_patch(rect)\n",
    "        \n",
    "        # Draw negative predictions (solid boxes)\n",
    "        if len(neg_pred_boxes) > 0 and neg_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            neg_pred_boxes_pixel = neg_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in neg_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width\n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none')\n",
    "                    axes[2].add_patch(rect)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate errors\n",
    "        pos_error = abs(pos_pred_count - len(positive_points))\n",
    "        neg_error = abs(neg_pred_count - len(negative_points))\n",
    "        total_error = pos_error + neg_error\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"  {positive_prompt} - GT: {len(positive_points)}, Pred: {pos_pred_count}, Error: {pos_error}\")\n",
    "        print(f\"  {negative_prompt} - GT: {len(negative_points)}, Pred: {neg_pred_count}, Error: {neg_error}\")\n",
    "        print(f\"  Total Error: {total_error}\")\n",
    "        print(f\"  Confidence Threshold: {confidence_thresh}\")\n",
    "        \n",
    "        return {\n",
    "            'positive_gt': len(positive_points),\n",
    "            'positive_pred': pos_pred_count,\n",
    "            'positive_error': pos_error,\n",
    "            'negative_gt': len(negative_points),\n",
    "            'negative_pred': neg_pred_count,\n",
    "            'negative_error': neg_error,\n",
    "            'total_error': total_error\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in CountGD evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"CountGD inference functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Replicate Results from Paper\n",
    "\n",
    "This section allows you to replicate the CountGD results from the PairTally paper.\n",
    "\n",
    "### Instructions:\n",
    "\n",
    "1. **Run single-class evaluation:**\n",
    "   ```bash\n",
    "   cd models/countgd\n",
    "   bash run_count_one_class.sh\n",
    "   ```\n",
    "\n",
    "2. **Run dual-class evaluation:**\n",
    "   ```bash\n",
    "   cd models/countgd\n",
    "   bash run_count_both_classes.sh\n",
    "   ```\n",
    "\n",
    "3. **Generate table row:**\n",
    "   After running both scripts, execute the cell below to replicate our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-Grained Counting Analysis for PairTally Dataset\n",
    "Based on professor_experiment.py - generates table row for CountGD results\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filename to extract object counts and types.\n",
    "    Format: {obj1_name}_{obj2_name}_{test_type}_{super_category}_{pos_code}_{neg_code}_{pos_count}_{neg_count}_{id1}_{id2}.jpg\n",
    "    \"\"\"\n",
    "    # Remove .jpg extension\n",
    "    name = filename.replace('.jpg', '')\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    if len(parts) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Find the indices for test_type, super_category, etc.\n",
    "    test_type_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in ['INTRA', 'INTER']:\n",
    "            test_type_idx = i\n",
    "            break\n",
    "    \n",
    "    if test_type_idx is None:\n",
    "        return None\n",
    "    \n",
    "    super_cat_idx = test_type_idx + 1\n",
    "    if super_cat_idx + 5 >= len(parts):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        pos_count = int(parts[super_cat_idx + 3])\n",
    "        neg_count = int(parts[super_cat_idx + 4])\n",
    "        return {\n",
    "            'pos_count': pos_count,  # count of category A (being asked about)\n",
    "            'neg_count': neg_count,  # count of category B (other category)\n",
    "            'total_count': pos_count + neg_count,\n",
    "            'test_type': parts[test_type_idx],\n",
    "            'super_category': parts[super_cat_idx]\n",
    "        }\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def calculate_fine_grained_metrics(qualitative_data_path, combined_data_path=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics needed for the fine-grained counting table:\n",
    "    1) |f(A) - a| - Error between single category prediction and true count\n",
    "    2) |f(A+B) - (a+b)| - Error between dual category prediction and total count  \n",
    "    3) |f(A) - (a+b)| - Error between single prediction and total count (overcounting check)\n",
    "    4) |f(A) - f(A+B)| - Difference between single and dual predictions\n",
    "    5) Percentage where f(A) > a (overcounting)\n",
    "    6) Percentage where |f(A) - a| > |f(A) - (a+b)| (overcounting indicator)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(qualitative_data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Load combined predictions if available\n",
    "    combined_predictions = {}\n",
    "    if combined_data_path and os.path.exists(combined_data_path):\n",
    "        try:\n",
    "            with open(combined_data_path, 'r') as f:\n",
    "                combined_data = json.load(f)\n",
    "                \n",
    "                # Handle different combined file formats\n",
    "                if 'image_results' in combined_data:\n",
    "                    # CountGD format - handle both regular and combined-text-only formats\n",
    "                    for filename, img_data in combined_data['image_results'].items():\n",
    "                        if 'combined_predicted_count' in img_data:\n",
    "                            # CountGD-Combined-TextOnly format\n",
    "                            combined_predictions[filename] = {'predicted_count': img_data['combined_predicted_count']}\n",
    "                        elif 'predicted_count' in img_data:\n",
    "                            # Regular CountGD format\n",
    "                            combined_predictions[filename] = {'predicted_count': img_data['predicted_count']}\n",
    "                elif 'results' in combined_data and 'images' in combined_data['results']:\n",
    "                    # DAVE format: convert to filename -> prediction mapping\n",
    "                    for img_data in combined_data['results']['images']:\n",
    "                        filename = img_data['image_name']\n",
    "                        pred_count = img_data['pred_total_count']\n",
    "                        combined_predictions[filename] = {'predicted_count': pred_count}\n",
    "                \n",
    "                if combined_predictions:\n",
    "                    print(f\"    Loaded {len(combined_predictions)} combined predictions\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not load combined data: {e}\")\n",
    "    \n",
    "    # Group predictions by image to pair single and dual category predictions\n",
    "    image_predictions = defaultdict(dict)\n",
    "    \n",
    "    # First pass: collect single category predictions (positive/negative)\n",
    "    for class_type in ['positive', 'negative']:\n",
    "        if class_type not in data['class_results']:\n",
    "            continue\n",
    "            \n",
    "        images = data['class_results'][class_type]['images']\n",
    "        \n",
    "        for image_data in images:\n",
    "            filename = image_data['image_name']\n",
    "            pred_count = image_data['pred_count']\n",
    "            gt_count = image_data['gt_count']\n",
    "            \n",
    "            # Parse filename to get true counts\n",
    "            parsed = parse_filename(filename)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            # Store prediction data\n",
    "            if filename not in image_predictions:\n",
    "                image_predictions[filename] = {\n",
    "                    'pos_count': parsed['pos_count'],  # a\n",
    "                    'neg_count': parsed['neg_count'],  # b  \n",
    "                    'total_count': parsed['total_count'],  # a + b\n",
    "                    'test_type': parsed['test_type'],\n",
    "                    'super_category': parsed['super_category']\n",
    "                }\n",
    "            \n",
    "            # Store the prediction for this category\n",
    "            if class_type == 'positive':\n",
    "                image_predictions[filename]['f_A'] = pred_count  # f(A) - single category prediction\n",
    "                image_predictions[filename]['a'] = parsed['pos_count']\n",
    "                image_predictions[filename]['b'] = parsed['neg_count']\n",
    "            else:\n",
    "                image_predictions[filename]['f_A'] = pred_count  # f(A) for negative category\n",
    "                image_predictions[filename]['a'] = parsed['neg_count']\n",
    "                image_predictions[filename]['b'] = parsed['pos_count']\n",
    "    \n",
    "    # Second pass: add combined predictions\n",
    "    for filename in image_predictions.keys():\n",
    "        if filename in combined_predictions:\n",
    "            combined_pred = combined_predictions[filename]['predicted_count']\n",
    "            image_predictions[filename]['f_A_plus_B'] = combined_pred\n",
    "    \n",
    "    # Calculate metrics for each image\n",
    "    results = []\n",
    "    \n",
    "    for filename, pred_data in image_predictions.items():\n",
    "        if 'f_A' not in pred_data:\n",
    "            continue\n",
    "            \n",
    "        f_A = pred_data['f_A']  # Single category prediction\n",
    "        a = pred_data['a']      # True count for category A\n",
    "        b = pred_data['b']      # True count for category B\n",
    "        a_plus_b = pred_data['total_count']  # Total count\n",
    "        \n",
    "        # Use actual dual-category prediction if available, otherwise use single prediction\n",
    "        f_A_plus_B = pred_data.get('f_A_plus_B', f_A)\n",
    "        \n",
    "        # Calculate required metrics\n",
    "        metrics = {\n",
    "            'filename': filename,\n",
    "            'f_A': f_A,\n",
    "            'f_A_plus_B': f_A_plus_B,\n",
    "            'a': a,\n",
    "            'b': b,\n",
    "            'a_plus_b': a_plus_b,\n",
    "            'error_f_A_minus_a': abs(f_A - a),                    # |f(A) - a|\n",
    "            'error_f_A_plus_B_minus_total': abs(f_A_plus_B - a_plus_b),  # |f(A+B) - (a+b)|\n",
    "            'error_f_A_minus_total': abs(f_A - a_plus_b),         # |f(A) - (a+b)|\n",
    "            'error_f_A_minus_f_A_plus_B': abs(f_A - f_A_plus_B),  # |f(A) - f(A+B)|\n",
    "            'f_A_greater_than_a': f_A > a,                        # f(A) > a\n",
    "            'overcounting_indicator': abs(f_A - a) > abs(f_A - a_plus_b),  # |f(A) - a| > |f(A) - (a+b)|\n",
    "            'test_type': pred_data['test_type'],\n",
    "            'super_category': pred_data['super_category']\n",
    "        }\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_fine_grained_metrics(results):\n",
    "    \"\"\"\n",
    "    Analyze the fine-grained metrics to generate table statistics.\n",
    "    Returns all metrics needed for the table.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    total_predictions = len(results)\n",
    "    \n",
    "    # Calculate mean errors\n",
    "    mean_f_A_minus_a = np.mean([r['error_f_A_minus_a'] for r in results])\n",
    "    mean_f_A_plus_B_minus_total = np.mean([r['error_f_A_plus_B_minus_total'] for r in results])\n",
    "    mean_f_A_minus_total = np.mean([r['error_f_A_minus_total'] for r in results])\n",
    "    mean_f_A_minus_f_A_plus_B = np.mean([r['error_f_A_minus_f_A_plus_B'] for r in results])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    f_A_greater_than_a_count = sum(1 for r in results if r['f_A_greater_than_a'])\n",
    "    f_A_greater_than_a_percentage = (f_A_greater_than_a_count / total_predictions) * 100\n",
    "    \n",
    "    overcounting_indicator_count = sum(1 for r in results if r['overcounting_indicator'])\n",
    "    overcounting_indicator_percentage = (overcounting_indicator_count / total_predictions) * 100\n",
    "    \n",
    "    return {\n",
    "        'total_predictions': total_predictions,\n",
    "        'mean_f_A_minus_a': mean_f_A_minus_a,\n",
    "        'mean_f_A_plus_B_minus_total': mean_f_A_plus_B_minus_total,\n",
    "        'mean_f_A_minus_total': mean_f_A_minus_total,\n",
    "        'mean_f_A_minus_f_A_plus_B': mean_f_A_minus_f_A_plus_B,\n",
    "        'f_A_greater_than_a_percentage': f_A_greater_than_a_percentage,\n",
    "        'overcounting_indicator_percentage': overcounting_indicator_percentage\n",
    "    }\n",
    "\n",
    "def generate_countgd_table_row():\n",
    "    \"\"\"\n",
    "    Generate table row for CountGD results on PairTally dataset.\n",
    "    Assumes the shell scripts have been run and results exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define result paths using relative paths (works from notebook directory)\n",
    "    # Single-class results path (from run_count_one_class.sh)\n",
    "    single_class_path = Path(\"results/CountGD-qualitative/final_dataset/complete_qualitative_data.json\")\n",
    "    \n",
    "    # Dual-class results path (from run_count_both_classes.sh) \n",
    "    # The script saves to ./outputs/${DATASET_NAME}_combined_predictions/\n",
    "    dual_class_path = Path(\"models/countgd/outputs/pairtally_dataset_combined_predictions/combined_inference_data.json\")\n",
    "    \n",
    "    print(\"Checking for required result files...\")\n",
    "    print(f\"  Single-class: {'Found' if single_class_path.exists() else 'Not Found'} {single_class_path}\")\n",
    "    print(f\"  Dual-class: {'Found' if dual_class_path.exists() else 'Not Found'} {dual_class_path}\")\n",
    "    \n",
    "    if not single_class_path.exists():\n",
    "        print(\"ERROR: Single-class results not found.\")\n",
    "        print(\"   Run from models/countgd: bash run_count_one_class.sh\")\n",
    "        return None\n",
    "        \n",
    "    if not dual_class_path.exists():\n",
    "        print(\"WARNING: Dual-class results not found. Will generate partial results.\")\n",
    "        print(\"   Run from models/countgd: bash run_count_both_classes.sh\")\n",
    "        dual_class_path = None\n",
    "    \n",
    "    try:\n",
    "        print(\"Calculating fine-grained metrics...\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fine_grained_data = calculate_fine_grained_metrics(str(single_class_path), str(dual_class_path) if dual_class_path else None)\n",
    "        \n",
    "        if not fine_grained_data:\n",
    "            print(\"ERROR: No valid data found for analysis\")\n",
    "            return None\n",
    "            \n",
    "        results = analyze_fine_grained_metrics(fine_grained_data)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"ERROR: Failed to analyze metrics\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"SUCCESS: Analyzed {len(fine_grained_data)} predictions\")\n",
    "        \n",
    "        # Generate table row\n",
    "        f_a_minus_a = results['mean_f_A_minus_a']\n",
    "        f_ab_minus_total = results['mean_f_A_plus_B_minus_total']\n",
    "        f_a_minus_total = results['mean_f_A_minus_total']\n",
    "        f_a_minus_f_ab = results['mean_f_A_minus_f_A_plus_B']\n",
    "        f_a_gt_a_pct = results['f_A_greater_than_a_percentage']\n",
    "        overcounting_pct = results['overcounting_indicator_percentage']\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nFINE-GRAINED COUNTING METRICS:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"|f(A) - a|: {f_a_minus_a:.2f}\")\n",
    "        print(f\"|f(A+B) - (a+b)|: {f_ab_minus_total:.2f}\")\n",
    "        print(f\"|f(A) - (a+b)|: {f_a_minus_total:.2f}\")\n",
    "        print(f\"|f(A) - f(A+B)|: {f_a_minus_f_ab:.2f}\")\n",
    "        print(f\"f(A) > a %: {f_a_gt_a_pct:.1f}\")\n",
    "        print(f\"|f(A) - a| > |f(A) - (a+b)| %: {overcounting_pct:.1f}\")\n",
    "        \n",
    "        # Generate LaTeX table row\n",
    "        latex_row = f\"CountGD & text & {f_a_minus_a:.2f} & {f_ab_minus_total:.2f} & {f_a_minus_total:.2f} & {f_a_minus_f_ab:.2f} & {f_a_gt_a_pct:.1f} & {overcounting_pct:.1f} \\\\\\\\\\\\\"\n",
    "        \n",
    "        print(\"\\nLATEX TABLE ROW:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(latex_row)\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error generating table row: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Fine-grained counting analysis functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Fine-Grained Counting Analysis for PairTally Dataset\n",
    "Based on professor_experiment.py - generates table row for CountGD results\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    Parse filename to extract object counts and types.\n",
    "    Format: {obj1_name}_{obj2_name}_{test_type}_{super_category}_{pos_code}_{neg_code}_{pos_count}_{neg_count}_{id1}_{id2}.jpg\n",
    "    \"\"\"\n",
    "    # Remove .jpg extension\n",
    "    name = filename.replace('.jpg', '')\n",
    "    parts = name.split('_')\n",
    "    \n",
    "    if len(parts) < 10:\n",
    "        return None\n",
    "    \n",
    "    # Find the indices for test_type, super_category, etc.\n",
    "    test_type_idx = None\n",
    "    for i, part in enumerate(parts):\n",
    "        if part in ['INTRA', 'INTER']:\n",
    "            test_type_idx = i\n",
    "            break\n",
    "    \n",
    "    if test_type_idx is None:\n",
    "        return None\n",
    "    \n",
    "    super_cat_idx = test_type_idx + 1\n",
    "    if super_cat_idx + 5 >= len(parts):\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        pos_count = int(parts[super_cat_idx + 3])\n",
    "        neg_count = int(parts[super_cat_idx + 4])\n",
    "        return {\n",
    "            'pos_count': pos_count,  # count of category A (being asked about)\n",
    "            'neg_count': neg_count,  # count of category B (other category)\n",
    "            'total_count': pos_count + neg_count,\n",
    "            'test_type': parts[test_type_idx],\n",
    "            'super_category': parts[super_cat_idx]\n",
    "        }\n",
    "    except (ValueError, IndexError):\n",
    "        return None\n",
    "\n",
    "def calculate_fine_grained_metrics(qualitative_data_path, combined_data_path=None):\n",
    "    \"\"\"\n",
    "    Calculate metrics needed for the fine-grained counting table:\n",
    "    1) |f(A) - a| - Error between single category prediction and true count\n",
    "    2) |f(A+B) - (a+b)| - Error between dual category prediction and total count  \n",
    "    3) |f(A) - (a+b)| - Error between single prediction and total count (overcounting check)\n",
    "    4) |f(A) - f(A+B)| - Difference between single and dual predictions\n",
    "    5) Percentage where f(A) > a (overcounting)\n",
    "    6) Percentage where |f(A) - a| > |f(A) - (a+b)| (overcounting indicator)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(qualitative_data_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Load combined predictions if available\n",
    "    combined_predictions = {}\n",
    "    if combined_data_path and os.path.exists(combined_data_path):\n",
    "        try:\n",
    "            with open(combined_data_path, 'r') as f:\n",
    "                combined_data = json.load(f)\n",
    "                \n",
    "                # Handle different combined file formats\n",
    "                if 'image_results' in combined_data:\n",
    "                    # CountGD format - handle both regular and combined-text-only formats\n",
    "                    for filename, img_data in combined_data['image_results'].items():\n",
    "                        if 'combined_predicted_count' in img_data:\n",
    "                            # CountGD-Combined-TextOnly format\n",
    "                            combined_predictions[filename] = {'predicted_count': img_data['combined_predicted_count']}\n",
    "                        elif 'predicted_count' in img_data:\n",
    "                            # Regular CountGD format\n",
    "                            combined_predictions[filename] = {'predicted_count': img_data['predicted_count']}\n",
    "                elif 'results' in combined_data and 'images' in combined_data['results']:\n",
    "                    # DAVE format: convert to filename -> prediction mapping\n",
    "                    for img_data in combined_data['results']['images']:\n",
    "                        filename = img_data['image_name']\n",
    "                        pred_count = img_data['pred_total_count']\n",
    "                        combined_predictions[filename] = {'predicted_count': pred_count}\n",
    "                \n",
    "                if combined_predictions:\n",
    "                    print(f\"    Loaded {len(combined_predictions)} combined predictions\")\n",
    "        except Exception as e:\n",
    "            print(f\"    Warning: Could not load combined data: {e}\")\n",
    "    \n",
    "    # Group predictions by image to pair single and dual category predictions\n",
    "    image_predictions = defaultdict(dict)\n",
    "    \n",
    "    # First pass: collect single category predictions (positive/negative)\n",
    "    for class_type in ['positive', 'negative']:\n",
    "        if class_type not in data['class_results']:\n",
    "            continue\n",
    "            \n",
    "        images = data['class_results'][class_type]['images']\n",
    "        \n",
    "        for image_data in images:\n",
    "            filename = image_data['image_name']\n",
    "            pred_count = image_data['pred_count']\n",
    "            gt_count = image_data['gt_count']\n",
    "            \n",
    "            # Parse filename to get true counts\n",
    "            parsed = parse_filename(filename)\n",
    "            if not parsed:\n",
    "                continue\n",
    "            \n",
    "            # Store prediction data\n",
    "            if filename not in image_predictions:\n",
    "                image_predictions[filename] = {\n",
    "                    'pos_count': parsed['pos_count'],  # a\n",
    "                    'neg_count': parsed['neg_count'],  # b  \n",
    "                    'total_count': parsed['total_count'],  # a + b\n",
    "                    'test_type': parsed['test_type'],\n",
    "                    'super_category': parsed['super_category']\n",
    "                }\n",
    "            \n",
    "            # Store the prediction for this category\n",
    "            if class_type == 'positive':\n",
    "                image_predictions[filename]['f_A'] = pred_count  # f(A) - single category prediction\n",
    "                image_predictions[filename]['a'] = parsed['pos_count']\n",
    "                image_predictions[filename]['b'] = parsed['neg_count']\n",
    "            else:\n",
    "                image_predictions[filename]['f_A'] = pred_count  # f(A) for negative category\n",
    "                image_predictions[filename]['a'] = parsed['neg_count']\n",
    "                image_predictions[filename]['b'] = parsed['pos_count']\n",
    "    \n",
    "    # Second pass: add combined predictions\n",
    "    for filename in image_predictions.keys():\n",
    "        if filename in combined_predictions:\n",
    "            combined_pred = combined_predictions[filename]['predicted_count']\n",
    "            image_predictions[filename]['f_A_plus_B'] = combined_pred\n",
    "    \n",
    "    # Calculate metrics for each image\n",
    "    results = []\n",
    "    \n",
    "    for filename, pred_data in image_predictions.items():\n",
    "        if 'f_A' not in pred_data:\n",
    "            continue\n",
    "            \n",
    "        f_A = pred_data['f_A']  # Single category prediction\n",
    "        a = pred_data['a']      # True count for category A\n",
    "        b = pred_data['b']      # True count for category B\n",
    "        a_plus_b = pred_data['total_count']  # Total count\n",
    "        \n",
    "        # Use actual dual-category prediction if available, otherwise use single prediction\n",
    "        f_A_plus_B = pred_data.get('f_A_plus_B', f_A)\n",
    "        \n",
    "        # Calculate required metrics\n",
    "        metrics = {\n",
    "            'filename': filename,\n",
    "            'f_A': f_A,\n",
    "            'f_A_plus_B': f_A_plus_B,\n",
    "            'a': a,\n",
    "            'b': b,\n",
    "            'a_plus_b': a_plus_b,\n",
    "            'error_f_A_minus_a': abs(f_A - a),                    # |f(A) - a|\n",
    "            'error_f_A_plus_B_minus_total': abs(f_A_plus_B - a_plus_b),  # |f(A+B) - (a+b)|\n",
    "            'error_f_A_minus_total': abs(f_A - a_plus_b),         # |f(A) - (a+b)|\n",
    "            'error_f_A_minus_f_A_plus_B': abs(f_A - f_A_plus_B),  # |f(A) - f(A+B)|\n",
    "            'f_A_greater_than_a': f_A > a,                        # f(A) > a\n",
    "            'overcounting_indicator': abs(f_A - a) > abs(f_A - a_plus_b),  # |f(A) - a| > |f(A) - (a+b)|\n",
    "            'test_type': pred_data['test_type'],\n",
    "            'super_category': pred_data['super_category']\n",
    "        }\n",
    "        \n",
    "        results.append(metrics)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def analyze_fine_grained_metrics(results):\n",
    "    \"\"\"\n",
    "    Analyze the fine-grained metrics to generate table statistics.\n",
    "    Returns all metrics needed for the table.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not results:\n",
    "        return None\n",
    "    \n",
    "    total_predictions = len(results)\n",
    "    \n",
    "    # Calculate mean errors\n",
    "    mean_f_A_minus_a = np.mean([r['error_f_A_minus_a'] for r in results])\n",
    "    mean_f_A_plus_B_minus_total = np.mean([r['error_f_A_plus_B_minus_total'] for r in results])\n",
    "    mean_f_A_minus_total = np.mean([r['error_f_A_minus_total'] for r in results])\n",
    "    mean_f_A_minus_f_A_plus_B = np.mean([r['error_f_A_minus_f_A_plus_B'] for r in results])\n",
    "    \n",
    "    # Calculate percentages\n",
    "    f_A_greater_than_a_count = sum(1 for r in results if r['f_A_greater_than_a'])\n",
    "    f_A_greater_than_a_percentage = (f_A_greater_than_a_count / total_predictions) * 100\n",
    "    \n",
    "    overcounting_indicator_count = sum(1 for r in results if r['overcounting_indicator'])\n",
    "    overcounting_indicator_percentage = (overcounting_indicator_count / total_predictions) * 100\n",
    "    \n",
    "    return {\n",
    "        'total_predictions': total_predictions,\n",
    "        'mean_f_A_minus_a': mean_f_A_minus_a,\n",
    "        'mean_f_A_plus_B_minus_total': mean_f_A_plus_B_minus_total,\n",
    "        'mean_f_A_minus_total': mean_f_A_minus_total,\n",
    "        'mean_f_A_minus_f_A_plus_B': mean_f_A_minus_f_A_plus_B,\n",
    "        'f_A_greater_than_a_percentage': f_A_greater_than_a_percentage,\n",
    "        'overcounting_indicator_percentage': overcounting_indicator_percentage\n",
    "    }\n",
    "\n",
    "def generate_countgd_table_row():\n",
    "    \"\"\"\n",
    "    Generate table row for CountGD results on PairTally dataset.\n",
    "    Assumes the shell scripts have been run and results exist.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define result paths using relative paths (works from notebook directory)\n",
    "    # Single-class results path (from run_count_one_class.sh)\n",
    "    single_class_path = Path(\"results/CountGD-qualitative/final_dataset/complete_qualitative_data.json\")\n",
    "    \n",
    "    # Dual-class results path (from run_count_both_classes.sh) \n",
    "    # The script saves to ./outputs/${DATASET_NAME}_combined_predictions/\n",
    "    dual_class_path = Path(\"models/countgd/outputs/pairtally_dataset_combined_predictions/combined_inference_data.json\")\n",
    "    \n",
    "    print(\"Checking for required result files...\")\n",
    "    print(f\"  Single-class: {'Found' if single_class_path.exists() else 'Not Found'} {single_class_path}\")\n",
    "    print(f\"  Dual-class: {'Found' if dual_class_path.exists() else 'Not Found'} {dual_class_path}\")\n",
    "    \n",
    "    if not single_class_path.exists():\n",
    "        print(\"ERROR: Single-class results not found.\")\n",
    "        print(\"   Run from models/countgd: bash run_count_one_class.sh\")\n",
    "        return None\n",
    "        \n",
    "    if not dual_class_path.exists():\n",
    "        print(\"WARNING: Dual-class results not found. Will generate partial results.\")\n",
    "        print(\"   Run from models/countgd: bash run_count_both_classes.sh\")\n",
    "        dual_class_path = None\n",
    "    \n",
    "    try:\n",
    "        print(\"Calculating fine-grained metrics...\")\n",
    "        \n",
    "        # Calculate metrics\n",
    "        fine_grained_data = calculate_fine_grained_metrics(str(single_class_path), str(dual_class_path) if dual_class_path else None)\n",
    "        \n",
    "        if not fine_grained_data:\n",
    "            print(\"ERROR: No valid data found for analysis\")\n",
    "            return None\n",
    "            \n",
    "        results = analyze_fine_grained_metrics(fine_grained_data)\n",
    "        \n",
    "        if not results:\n",
    "            print(\"ERROR: Failed to analyze metrics\")\n",
    "            return None\n",
    "            \n",
    "        print(f\"SUCCESS: Analyzed {len(fine_grained_data)} predictions\")\n",
    "        \n",
    "        # Generate table row\n",
    "        f_a_minus_a = results['mean_f_A_minus_a']\n",
    "        f_ab_minus_total = results['mean_f_A_plus_B_minus_total']\n",
    "        f_a_minus_total = results['mean_f_A_minus_total']\n",
    "        f_a_minus_f_ab = results['mean_f_A_minus_f_A_plus_B']\n",
    "        f_a_gt_a_pct = results['f_A_greater_than_a_percentage']\n",
    "        overcounting_pct = results['overcounting_indicator_percentage']\n",
    "        \n",
    "        # Print results\n",
    "        print(\"\\nFINE-GRAINED COUNTING METRICS:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"|f(A) - a|: {f_a_minus_a:.2f}\")\n",
    "        print(f\"|f(A+B) - (a+b)|: {f_ab_minus_total:.2f}\")\n",
    "        print(f\"|f(A) - (a+b)|: {f_a_minus_total:.2f}\")\n",
    "        print(f\"|f(A) - f(A+B)|: {f_a_minus_f_ab:.2f}\")\n",
    "        print(f\"f(A) > a %: {f_a_gt_a_pct:.1f}\")\n",
    "        print(f\"|f(A) - a| > |f(A) - (a+b)| %: {overcounting_pct:.1f}\")\n",
    "        \n",
    "        # Generate LaTeX table row\n",
    "        latex_row = f\"CountGD & text & {f_a_minus_a:.2f} & {f_ab_minus_total:.2f} & {f_a_minus_total:.2f} & {f_a_minus_f_ab:.2f} & {f_a_gt_a_pct:.1f} & {overcounting_pct:.1f} \\\\\\\\\\\\\"\n",
    "        \n",
    "        print(\"\\nLATEX TABLE ROW:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(latex_row)\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: Error generating table row: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"Fine-grained counting analysis functions loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to generate the table row for your paper\n",
    "# This assumes you have already run the two shell scripts above\n",
    "\n",
    "results = generate_countgd_table_row()\n",
    "\n",
    "if results:\n",
    "    print(\"Successfully generated CountGD table row!\")\n",
    "    print(\"You can copy the LaTeX row above directly into your paper.\")\n",
    "else:\n",
    "    print(\"Failed to generate table row. Make sure you've run both shell scripts first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "countgd_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
