{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PairTally Dataset Demonstration\n",
    "\n",
    "## Overview\n",
    "\n",
    "PairTally is the first benchmark specifically designed to evaluate fine-grained visual counting capabilities in computer vision models. This notebook provides a comprehensive demonstration of the dataset structure, visualization capabilities, and evaluation pipeline.\n",
    "\n",
    "### Paper\n",
    "\n",
    "**Can Current AI Models Count What We Mean, Not What They See? A Benchmark and Systematic Evaluation**  \n",
    "Gia Khanh Nguyen, Yifeng Huang, Minh Hoai  \n",
    "Digital Image Computing: Techniques and Applications (DICTA) 2025\n",
    "\n",
    "### Dataset Specifications\n",
    "\n",
    "- **Total Images**: 681 high-resolution images\n",
    "- **Categories**: 54 object categories across 98 subcategories\n",
    "- **Task Types**: Inter-category (different objects) and Intra-category (same object, different attributes)\n",
    "- **Attribute Differences**: Color (43.5%), Shape/Texture (42.5%), Size (14.0%)\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "Current state-of-the-art models achieve Mean Absolute Error (MAE) of 53.07, revealing critical gaps in fine-grained visual understanding and discrimination capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Rectangle\n",
    "import seaborn as sns\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment configured successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure dataset paths\n",
    "BASE_DIR = Path(\".\").resolve()\n",
    "DATASET_DIR = BASE_DIR / \"dataset\" / \"pairtally_dataset\"\n",
    "ANNOTATIONS_DIR = DATASET_DIR / \"annotations\"\n",
    "IMAGES_DIR = DATASET_DIR / \"images\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "def verify_dataset():\n",
    "    \"\"\"Verify dataset structure and availability\"\"\"\n",
    "    issues = []\n",
    "    \n",
    "    if not DATASET_DIR.exists():\n",
    "        issues.append(f\"Dataset directory not found: {DATASET_DIR}\")\n",
    "    \n",
    "    if not ANNOTATIONS_DIR.exists():\n",
    "        issues.append(f\"Annotations directory not found: {ANNOTATIONS_DIR}\")\n",
    "    \n",
    "    if not IMAGES_DIR.exists():\n",
    "        issues.append(f\"Images directory not found: {IMAGES_DIR}\")\n",
    "        issues.append(\"Download images from: https://drive.google.com/file/d/1TnenXS4yFicjo81NnmClfzgc8ltmmeBv/view\")\n",
    "    \n",
    "    if issues:\n",
    "        for issue in issues:\n",
    "            print(f\"ERROR: {issue}\")\n",
    "        return False\n",
    "    \n",
    "    # Count images\n",
    "    num_images = len(list(IMAGES_DIR.glob(\"*.jpg\")))\n",
    "    print(f\"Dataset verified successfully\")\n",
    "    print(f\"Found {num_images} images at {DATASET_DIR}\")\n",
    "    return True\n",
    "\n",
    "dataset_ready = verify_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Dataset Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PairTallyDataset:\n",
    "    \"\"\"PairTally dataset interface for loading and accessing annotations\"\"\"\n",
    "    \n",
    "    def __init__(self, dataset_dir: Path, version: str = \"simple\"):\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.annotations_dir = dataset_dir / \"annotations\"\n",
    "        self.images_dir = dataset_dir / \"images\"\n",
    "        self.version = version  # \"simple\" or \"augmented\"\n",
    "        \n",
    "        # Load annotations\n",
    "        self.annotations = self._load_annotations()\n",
    "        self.inter_annotations = self._load_inter_annotations()\n",
    "        self.intra_annotations = self._load_intra_annotations()\n",
    "        \n",
    "        # Load metadata\n",
    "        self.metadata = self._load_metadata()\n",
    "        self.filename_mapping = self._load_filename_mapping()\n",
    "        \n",
    "        print(f\"Loaded {len(self.annotations)} total annotations\")\n",
    "        print(f\"  Inter-category: {len(self.inter_annotations)} images\")\n",
    "        print(f\"  Intra-category: {len(self.intra_annotations)} images\")\n",
    "    \n",
    "    def _load_json(self, filename: str) -> Dict:\n",
    "        \"\"\"Load a JSON file\"\"\"\n",
    "        filepath = self.annotations_dir / filename\n",
    "        if filepath.exists():\n",
    "            with open(filepath, 'r') as f:\n",
    "                return json.load(f)\n",
    "        return {}\n",
    "    \n",
    "    def _load_annotations(self) -> Dict:\n",
    "        \"\"\"Load main annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_inter_annotations(self) -> Dict:\n",
    "        \"\"\"Load inter-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_inter_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_intra_annotations(self) -> Dict:\n",
    "        \"\"\"Load intra-category annotations\"\"\"\n",
    "        filename = f\"pairtally_annotations_intra_{self.version}.json\"\n",
    "        return self._load_json(filename)\n",
    "    \n",
    "    def _load_metadata(self) -> Dict:\n",
    "        \"\"\"Load image metadata\"\"\"\n",
    "        return self._load_json(\"image_metadata.json\")\n",
    "    \n",
    "    def _load_filename_mapping(self) -> Dict:\n",
    "        \"\"\"Load filename mapping\"\"\"\n",
    "        return self._load_json(\"filename_mapping.json\")\n",
    "    \n",
    "    def get_random_image(self, subset: str = \"all\") -> str:\n",
    "        \"\"\"Get a random image filename from the dataset\"\"\"\n",
    "        if subset == \"inter\":\n",
    "            images = list(self.inter_annotations.keys())\n",
    "        elif subset == \"intra\":\n",
    "            images = list(self.intra_annotations.keys())\n",
    "        else:\n",
    "            images = list(self.annotations.keys())\n",
    "        \n",
    "        return random.choice(images) if images else None\n",
    "    \n",
    "    def get_annotation(self, image_name: str) -> Dict:\n",
    "        \"\"\"Get annotation for a specific image\"\"\"\n",
    "        return self.annotations.get(image_name, {})\n",
    "    \n",
    "    def get_counts(self, image_name: str) -> Tuple[int, int]:\n",
    "        \"\"\"Get positive and negative class counts for an image\"\"\"\n",
    "        anno = self.get_annotation(image_name)\n",
    "        if anno:\n",
    "            positive_count = len(anno.get('points', []))\n",
    "            negative_count = len(anno.get('negative_points', []))\n",
    "            return positive_count, negative_count\n",
    "        return 0, 0\n",
    "    \n",
    "    def get_image_path(self, image_name: str) -> Path:\n",
    "        \"\"\"Get full path to an image\"\"\"\n",
    "        return self.images_dir / image_name\n",
    "\n",
    "# Initialize dataset\n",
    "if dataset_ready:\n",
    "    dataset = PairTallyDataset(DATASET_DIR, version=\"simple\")\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Total images: {len(dataset.annotations)}\")\n",
    "    print(f\"Inter-category pairs: {len(dataset.inter_annotations)}\")\n",
    "    print(f\"Intra-category pairs: {len(dataset.intra_annotations)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_image_with_annotations(dataset: PairTallyDataset, \n",
    "                                    image_name: str,\n",
    "                                    show_boxes: bool = True,\n",
    "                                    show_points: bool = False,\n",
    "                                    figsize: Tuple[int, int] = None):\n",
    "    \"\"\"\n",
    "    Visualize an image with bounding box annotations and object counts.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        image_name: Name of the image to visualize\n",
    "        show_boxes: Whether to show bounding box examples\n",
    "        show_points: Whether to show point annotations\n",
    "        figsize: Figure size for visualization\n",
    "    \"\"\"\n",
    "    # Get image path and annotation\n",
    "    img_path = dataset.get_image_path(image_name)\n",
    "    annotation = dataset.get_annotation(image_name)\n",
    "    \n",
    "    if not img_path.exists():\n",
    "        print(f\"ERROR: Image not found: {img_path}\")\n",
    "        return\n",
    "    \n",
    "    # Load image\n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    \n",
    "    # Get image dimensions\n",
    "    height, width = img_array.shape[:2]\n",
    "    \n",
    "    # Auto-calculate figure size to maintain aspect ratio\n",
    "    if figsize is None:\n",
    "        aspect_ratio = width / height\n",
    "        fig_width = 10\n",
    "        fig_height = fig_width / aspect_ratio + 1\n",
    "        figsize = (fig_width, fig_height)\n",
    "    \n",
    "    # Get annotation data\n",
    "    positive_points = annotation.get('points', [])\n",
    "    negative_points = annotation.get('negative_points', [])\n",
    "    positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "    negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "    positive_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "    negative_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=figsize, facecolor='white')\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(img_array)\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Add title with counts\n",
    "    blue_text = f\"{positive_prompt}: {len(positive_points)}\"\n",
    "    red_text = f\"{negative_prompt}: {len(negative_points)}\"\n",
    "    \n",
    "    fig.text(0.45, 0.95, blue_text, \n",
    "             ha='right', va='top', fontsize=14, color='#0066FF', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.5, 0.95, \" | \", \n",
    "             ha='center', va='top', fontsize=14, color='#666666', \n",
    "             fontweight='normal', transform=fig.transFigure)\n",
    "    \n",
    "    fig.text(0.55, 0.95, red_text, \n",
    "             ha='left', va='top', fontsize=14, color='#FF0040', \n",
    "             fontweight='bold', transform=fig.transFigure)\n",
    "    \n",
    "    # Plot bounding boxes\n",
    "    if show_boxes:\n",
    "        # Positive class boxes in blue\n",
    "        for box_coords in positive_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#0066FF', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "        \n",
    "        # Negative class boxes in red\n",
    "        for box_coords in negative_boxes[:3]:\n",
    "            if len(box_coords) == 4:\n",
    "                x_coords = [pt[0] for pt in box_coords]\n",
    "                y_coords = [pt[1] for pt in box_coords]\n",
    "                x1, x2 = min(x_coords), max(x_coords)\n",
    "                y1, y2 = min(y_coords), max(y_coords)\n",
    "                \n",
    "                rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                               linewidth=2, edgecolor='#FF0040', \n",
    "                               facecolor='none')\n",
    "                ax.add_patch(rect)\n",
    "    \n",
    "    # Plot points if requested\n",
    "    if show_points:\n",
    "        if positive_points:\n",
    "            pos_points = np.array(positive_points)\n",
    "            ax.scatter(pos_points[:, 0], pos_points[:, 1], \n",
    "                      c='#0066FF', s=8, alpha=0.4, marker='.')\n",
    "        \n",
    "        if negative_points:\n",
    "            neg_points = np.array(negative_points)\n",
    "            ax.scatter(neg_points[:, 0], neg_points[:, 1], \n",
    "                      c='#FF0040', s=8, alpha=0.4, marker='.')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.02, 1, 0.94])\n",
    "    plt.show()\n",
    "    \n",
    "    return positive_points, negative_points\n",
    "\n",
    "def visualize_three_images(dataset: PairTallyDataset, \n",
    "                          image_names: List[str],\n",
    "                          show_boxes: bool = True,\n",
    "                          show_points: bool = False):\n",
    "    \"\"\"\n",
    "    Visualize exactly 3 images in a horizontal layout.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        image_names: List of exactly 3 image names to visualize\n",
    "        show_boxes: Whether to show bounding box examples\n",
    "        show_points: Whether to show point annotations\n",
    "    \"\"\"\n",
    "    if len(image_names) != 3:\n",
    "        print(f\"ERROR: This function requires exactly 3 images, got {len(image_names)}\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with 3 horizontal subplots\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6), facecolor='white')\n",
    "    \n",
    "    for i, image_name in enumerate(image_names):\n",
    "        # Get image path and annotation\n",
    "        img_path = dataset.get_image_path(image_name)\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"ERROR: Image not found: {img_path}\")\n",
    "            axes[i].text(0.5, 0.5, f\"Image not found:\\n{image_name}\", \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].axis('off')\n",
    "            continue\n",
    "        \n",
    "        # Load image\n",
    "        img = Image.open(img_path)\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Get annotation data\n",
    "        positive_points = annotation.get('points', [])\n",
    "        negative_points = annotation.get('negative_points', [])\n",
    "        positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "        negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "        positive_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        negative_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        \n",
    "        # Display image\n",
    "        axes[i].imshow(img_array)\n",
    "        axes[i].axis('off')\n",
    "        \n",
    "        # Add title with colored text\n",
    "        blue_text = f\"{positive_prompt}: {len(positive_points)}\"\n",
    "        red_text = f\"{negative_prompt}: {len(negative_points)}\"\n",
    "        \n",
    "        # Position title above each subplot\n",
    "        fig.text(0.17 + i*0.33, 0.92, blue_text, \n",
    "                ha='center', va='top', fontsize=12, color='#0066FF', \n",
    "                fontweight='bold', transform=fig.transFigure)\n",
    "        \n",
    "        fig.text(0.17 + i*0.33, 0.88, red_text, \n",
    "                ha='center', va='top', fontsize=12, color='#FF0040', \n",
    "                fontweight='bold', transform=fig.transFigure)\n",
    "        \n",
    "        # Add image filename at bottom\n",
    "        axes[i].text(0.5, -0.05, image_name, \n",
    "                    ha='center', va='top', fontsize=10, \n",
    "                    transform=axes[i].transAxes, weight='bold')\n",
    "        \n",
    "        # Plot bounding boxes\n",
    "        if show_boxes:\n",
    "            # Positive class boxes in blue\n",
    "            for box_coords in positive_boxes[:3]:\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor='#0066FF', \n",
    "                                   facecolor='none')\n",
    "                    axes[i].add_patch(rect)\n",
    "            \n",
    "            # Negative class boxes in red\n",
    "            for box_coords in negative_boxes[:3]:\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    \n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                   linewidth=2, edgecolor='#FF0040', \n",
    "                                   facecolor='none')\n",
    "                    axes[i].add_patch(rect)\n",
    "        \n",
    "        # Plot points if requested\n",
    "        if show_points:\n",
    "            if positive_points:\n",
    "                pos_points = np.array(positive_points)\n",
    "                axes[i].scatter(pos_points[:, 0], pos_points[:, 1], \n",
    "                              c='#0066FF', s=8, alpha=0.4, marker='.')\n",
    "            \n",
    "            if negative_points:\n",
    "                neg_points = np.array(negative_points)\n",
    "                axes[i].scatter(neg_points[:, 0], neg_points[:, 1], \n",
    "                              c='#FF0040', s=8, alpha=0.4, marker='.')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0.05, 1, 0.85])\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_statistics(dataset: PairTallyDataset):\n",
    "    \"\"\"Display comprehensive statistics about the PairTally dataset\"\"\"\n",
    "    \n",
    "    # Collect statistics\n",
    "    all_positive_counts = []\n",
    "    all_negative_counts = []\n",
    "    all_total_counts = []\n",
    "    \n",
    "    for img_name in dataset.annotations.keys():\n",
    "        pos_count, neg_count = dataset.get_counts(img_name)\n",
    "        all_positive_counts.append(pos_count)\n",
    "        all_negative_counts.append(neg_count)\n",
    "        all_total_counts.append(pos_count + neg_count)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.patch.set_facecolor('white')\n",
    "    \n",
    "    # Distribution of total counts\n",
    "    axes[0, 0].hist(all_total_counts, bins=30, color='#2E86AB', edgecolor='#1B4F72', alpha=0.8)\n",
    "    axes[0, 0].set_title('Distribution of Total Object Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Total Count')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].axvline(np.mean(all_total_counts), color='#E74C3C', linestyle='--', linewidth=2,\n",
    "                      label=f'Mean: {np.mean(all_total_counts):.1f}')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of positive vs negative counts\n",
    "    axes[0, 1].hist([all_positive_counts, all_negative_counts], \n",
    "                   bins=25, label=['Positive Class', 'Negative Class'],\n",
    "                   color=['#0066FF', '#FF0040'], alpha=0.7, edgecolor='#2C3E50')\n",
    "    axes[0, 1].set_title('Distribution of Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Count')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of positive vs negative counts\n",
    "    axes[1, 0].scatter(all_positive_counts, all_negative_counts, \n",
    "                      alpha=0.6, s=25, c='#8E44AD', edgecolors='#5B2C6F', linewidth=0.5)\n",
    "    axes[1, 0].set_title('Positive vs Negative Class Counts', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Positive Class Count')\n",
    "    axes[1, 0].set_ylabel('Negative Class Count')\n",
    "    axes[1, 0].plot([0, max(all_positive_counts)], [0, max(all_positive_counts)], \n",
    "                   'k--', alpha=0.3, label='Equal counts', linewidth=1)\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Summary statistics\n",
    "    stats_text = f\"\"\"Dataset Summary Statistics\n",
    "    \n",
    "Total Images: {len(dataset.annotations)}\n",
    "Inter-category: {len(dataset.inter_annotations)}\n",
    "Intra-category: {len(dataset.intra_annotations)}\n",
    "\n",
    "Object Count Statistics:\n",
    "  Mean total: {np.mean(all_total_counts):.1f} ± {np.std(all_total_counts):.1f}\n",
    "  Min/Max: {min(all_total_counts)} / {max(all_total_counts)}\n",
    "  Median: {np.median(all_total_counts):.1f}\n",
    "\n",
    "Positive Class:\n",
    "  Mean: {np.mean(all_positive_counts):.1f} ± {np.std(all_positive_counts):.1f}\n",
    "  Range: [{min(all_positive_counts)}, {max(all_positive_counts)}]\n",
    "\n",
    "Negative Class:\n",
    "  Mean: {np.mean(all_negative_counts):.1f} ± {np.std(all_negative_counts):.1f}\n",
    "  Range: [{min(all_negative_counts)}, {max(all_negative_counts)}]\n",
    "\"\"\"\n",
    "    \n",
    "    axes[1, 1].text(0.1, 0.5, stats_text, transform=axes[1, 1].transAxes,\n",
    "                   fontsize=11, verticalalignment='center', fontfamily='monospace',\n",
    "                   bbox=dict(boxstyle='round,pad=0.8', facecolor='#F8F9FA', alpha=0.9, edgecolor='#BDC3C7'))\n",
    "    axes[1, 1].axis('off')\n",
    "    \n",
    "    plt.suptitle('PairTally Dataset Statistics', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Display statistics\n",
    "if dataset_ready:\n",
    "    print(\"Generating dataset statistics visualization...\")\n",
    "    display_dataset_statistics(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demo: Select and Visualize Random Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demo_random_images(dataset: PairTallyDataset, \n",
    "                       num_images: int = 6,\n",
    "                       subset: str = \"all\"):\n",
    "    \"\"\"\n",
    "    Demonstrate dataset visualization with random image selection.\n",
    "    Show images in groups of 3 to reduce scrolling.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        num_images: Number of random images to display\n",
    "        subset: Which subset to use (\"all\", \"inter\", \"intra\")\n",
    "    \"\"\"\n",
    "    print(f\"\\nSelecting {num_images} random images from {subset} subset...\\n\")\n",
    "    \n",
    "    # Get random images\n",
    "    selected_images = []\n",
    "    for i in range(num_images):\n",
    "        image_name = dataset.get_random_image(subset=subset)\n",
    "        if image_name and image_name not in selected_images:\n",
    "            selected_images.append(image_name)\n",
    "        elif image_name in selected_images:\n",
    "            # Try again with different random seed\n",
    "            for _ in range(10):  # Max 10 retries\n",
    "                alt_image = dataset.get_random_image(subset=subset)\n",
    "                if alt_image and alt_image not in selected_images:\n",
    "                    selected_images.append(alt_image)\n",
    "                    break\n",
    "    \n",
    "    if not selected_images:\n",
    "        print(\"No images found\")\n",
    "        return\n",
    "    \n",
    "    # Display info about selected images\n",
    "    for i, image_name in enumerate(selected_images):\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        pos_count, neg_count = dataset.get_counts(image_name)\n",
    "        pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        category_type = \"INTER\" if image_name in dataset.inter_annotations else \"INTRA\"\n",
    "        \n",
    "        print(f\"Image {i+1}: {image_name[:40]}\")\n",
    "        print(f\"  Type: {category_type} | Classes: {pos_prompt} vs {neg_prompt} | Counts: {pos_count}+{neg_count}={pos_count+neg_count}\")\n",
    "    \n",
    "    print(\"\\\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Display images in groups of 3\n",
    "    for i in range(0, len(selected_images), 3):\n",
    "        group = selected_images[i:i+3]\n",
    "        \n",
    "        if len(group) == 3:\n",
    "            print(f\"\\\\nGroup {i//3 + 1}: Images {i+1}-{i+3}\")\n",
    "            visualize_three_images(dataset, group, show_boxes=True, show_points=False)\n",
    "        else:\n",
    "            # For remaining images that don't make a group of 3\n",
    "            print(f\"\\\\nRemaining images:\")\n",
    "            for img_name in group:\n",
    "                visualize_image_with_annotations(dataset, img_name, \n",
    "                                               show_boxes=True, show_points=False)\n",
    "\n",
    "def demo_specific_attribute_images(dataset: PairTallyDataset, \n",
    "                                  attribute_type: str = \"color\",\n",
    "                                  num_examples: int = 3):\n",
    "    \"\"\"\n",
    "    Demonstrate specific attribute-based intra-category examples.\n",
    "    \n",
    "    Args:\n",
    "        dataset: PairTallyDataset instance\n",
    "        attribute_type: Type of attribute difference (\"color\", \"size\", \"texture\")\n",
    "        num_examples: Number of examples to show (will show exactly 3)\n",
    "    \"\"\"\n",
    "    # Find intra-category images with specific patterns\n",
    "    intra_images = list(dataset.intra_annotations.keys())\n",
    "    candidates = []\n",
    "    \n",
    "    if attribute_type == \"color\":\n",
    "        keywords = ['COL', 'color', 'black', 'white', 'red', 'blue', 'green', 'yellow']\n",
    "    elif attribute_type == \"size\":\n",
    "        keywords = ['SIZ', 'size', 'big', 'small', 'large', 'tiny']\n",
    "    else:  # texture/shape\n",
    "        keywords = ['TEX', 'SHA', 'round', 'square', 'smooth', 'rough']\n",
    "    \n",
    "    # Filter images that might match\n",
    "    for img in intra_images:\n",
    "        if any(kw.lower() in img.lower() for kw in keywords):\n",
    "            candidates.append(img)\n",
    "    \n",
    "    # If no specific matches, just use any intra images\n",
    "    if not candidates:\n",
    "        candidates = intra_images\n",
    "    \n",
    "    # Select exactly 3 examples\n",
    "    selected = random.sample(candidates, min(3, len(candidates)))\n",
    "    \n",
    "    print(f\"\\\\n{attribute_type.upper()} DIFFERENCE EXAMPLES:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Display info\n",
    "    for i, image_name in enumerate(selected):\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        pos_count, neg_count = dataset.get_counts(image_name)\n",
    "        pos_prompt = annotation.get('positive_prompt', 'Class 1')\n",
    "        neg_prompt = annotation.get('negative_prompt', 'Class 2')\n",
    "        \n",
    "        print(f\"Example {i+1}: {image_name[:40]}\")\n",
    "        print(f\"  Classes: {pos_prompt} vs {neg_prompt} | Counts: {pos_count}+{neg_count}={pos_count+neg_count}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Visualize as a group of 3\n",
    "    if len(selected) == 3:\n",
    "        visualize_three_images(dataset, selected, show_boxes=True, show_points=False)\n",
    "    else:\n",
    "        # Fallback for less than 3 images\n",
    "        for image_name in selected:\n",
    "            visualize_image_with_annotations(dataset, image_name, \n",
    "                                           show_boxes=True, show_points=False)\n",
    "\n",
    "# Run comprehensive demonstration\n",
    "if dataset_ready:\n",
    "    print(\"=\"*80)\n",
    "    print(\"COMPREHENSIVE DATASET VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Show 6 INTER-category examples (2 groups of 3)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"1. INTER-CATEGORY EXAMPLES (Different Object Types)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=6, subset=\"inter\")\n",
    "    \n",
    "    # 2. Show 6 INTRA-category examples (2 groups of 3)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"2. INTRA-CATEGORY EXAMPLES (Same Object, Different Attributes)\")\n",
    "    print(\"=\"*60)\n",
    "    demo_random_images(dataset, num_images=6, subset=\"intra\")\n",
    "    \n",
    "    # 3. Show specific attribute differences (3 examples each)\n",
    "    print(\"\\\\n\" + \"=\"*60)\n",
    "    print(\"3. SPECIFIC ATTRIBUTE DIFFERENCES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    print(\"Demonstrating attribute-based discrimination challenges:\")\n",
    "    \n",
    "    # Color difference examples\n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"color\", \n",
    "                                 num_examples=3)\n",
    "    \n",
    "    # Size difference examples  \n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"size\", \n",
    "                                 num_examples=3)\n",
    "    \n",
    "    # Texture/Shape difference examples\n",
    "    demo_specific_attribute_images(dataset, attribute_type=\"texture\", \n",
    "                                 num_examples=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. CountGD Model Evaluation Demo\n",
    "\n",
    "This section demonstrates how to use the CountGD model on PairTally dataset images. CountGD is a state-of-the-art object counting model that can use both exemplar boxes and text prompts for counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import CountGD model components\n",
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import torchvision.transforms.functional as F\n",
    "\n",
    "# Add CountGD to path\n",
    "countgd_path = BASE_DIR / \"models\" / \"countgd\" / \"CountGD\"\n",
    "sys.path.append(str(countgd_path))\n",
    "\n",
    "try:\n",
    "    from util.slconfig import SLConfig, DictAction\n",
    "    from util.misc import nested_tensor_from_tensor_list\n",
    "    import datasets_inference.transforms as T\n",
    "    from models.registry import MODULE_BUILD_FUNCS\n",
    "    \n",
    "    print(\"CountGD imports successful\")\n",
    "    countgd_available = True\n",
    "except ImportError as e:\n",
    "    print(f\"CountGD import failed: {e}\")\n",
    "    print(\"Make sure CountGD dependencies are installed\")\n",
    "    countgd_available = False\n",
    "\n",
    "def load_countgd_model():\n",
    "    \"\"\"Load the CountGD model with default configuration\"\"\"\n",
    "    if not countgd_available:\n",
    "        print(\"CountGD not available - skipping model loading\")\n",
    "        return None, None\n",
    "        \n",
    "    # Set default paths\n",
    "    config_path = countgd_path / \"config\" / \"cfg_fsc147_vit_b.py\"\n",
    "    checkpoint_path = countgd_path / \"checkpoints\" / \"checkpoint_fsc147_best.pth\"\n",
    "    \n",
    "    if not config_path.exists():\n",
    "        print(f\"Config file not found: {config_path}\")\n",
    "        print(\"Please ensure CountGD model files are properly set up\")\n",
    "        return None, None\n",
    "        \n",
    "    if not checkpoint_path.exists():\n",
    "        print(f\"Checkpoint file not found: {checkpoint_path}\")\n",
    "        print(\"Please download the CountGD model checkpoint\")\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        # Load config\n",
    "        cfg = SLConfig.fromfile(str(config_path))\n",
    "        cfg.merge_from_dict({\"text_encoder_type\": str(countgd_path / \"checkpoints\" / \"bert-base-uncased\")})\n",
    "        \n",
    "        # Set device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        \n",
    "        # Build model\n",
    "        build_func = MODULE_BUILD_FUNCS.get(\"groundingdino\")\n",
    "        if build_func is None:\n",
    "            print(\"Model builder not found\")\n",
    "            return None, None\n",
    "            \n",
    "        class Args:\n",
    "            def __init__(self):\n",
    "                self.modelname = \"groundingdino\"\n",
    "                self.device = str(device)\n",
    "        \n",
    "        args = Args()\n",
    "        for k, v in cfg._cfg_dict.items():\n",
    "            setattr(args, k, v)\n",
    "            \n",
    "        model, _, _ = build_func(args)\n",
    "        model.to(device)\n",
    "        \n",
    "        # Load checkpoint - FIX: Set weights_only=False for PyTorch 2.6+\n",
    "        print(\"Loading checkpoint...\")\n",
    "        checkpoint = torch.load(str(checkpoint_path), map_location=device, weights_only=False)[\"model\"]\n",
    "        model.load_state_dict(checkpoint, strict=False)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create transform\n",
    "        transforms = T.Compose([\n",
    "            T.RandomResize([800], max_size=1333),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "        print(\"CountGD model loaded successfully!\")\n",
    "        return model, transforms\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load CountGD model: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None, None\n",
    "\n",
    "# Load the model\n",
    "if countgd_available:\n",
    "    print(\"Loading CountGD model...\")\n",
    "    countgd_model, countgd_transform = load_countgd_model()\n",
    "    model_ready = (countgd_model is not None)\n",
    "else:\n",
    "    model_ready = False\n",
    "    print(\"CountGD model not available for this demo\")\n",
    "\n",
    "print(f\"Model ready status: {model_ready}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_countgd_inference(model, transform, image, exemplar_boxes, text_prompt, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD inference on an image with exemplar boxes and text prompt\n",
    "    \n",
    "    Args:\n",
    "        model: CountGD model\n",
    "        transform: Image transform pipeline\n",
    "        image: PIL Image\n",
    "        exemplar_boxes: List of exemplar boxes in [x1, y1, x2, y2] format\n",
    "        text_prompt: Text description of objects to count\n",
    "        confidence_thresh: Confidence threshold for detections\n",
    "        \n",
    "    Returns:\n",
    "        pred_boxes: Predicted bounding boxes (normalized coordinates)\n",
    "        pred_logits: Prediction logits/scores\n",
    "        pred_count: Number of predicted objects\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # Convert exemplar boxes to tensor format if available\n",
    "    if exemplar_boxes and len(exemplar_boxes) > 0:\n",
    "        # Convert to normalized coordinates relative to image size\n",
    "        img_width, img_height = image.size\n",
    "        exemplar_tensor = []\n",
    "        for box in exemplar_boxes[:3]:  # Limit to 3 exemplars\n",
    "            x1, y1, x2, y2 = box\n",
    "            # Normalize to [0, 1]\n",
    "            norm_x1 = x1 / img_width\n",
    "            norm_y1 = y1 / img_height\n",
    "            norm_x2 = x2 / img_width\n",
    "            norm_y2 = y2 / img_height\n",
    "            exemplar_tensor.append([norm_x1, norm_y1, norm_x2, norm_y2])\n",
    "        \n",
    "        exemplar_tensor = torch.tensor(exemplar_tensor, dtype=torch.float32)\n",
    "    else:\n",
    "        # Use empty tensor if no exemplars\n",
    "        exemplar_tensor = torch.tensor([], dtype=torch.float32).reshape(0, 4)\n",
    "    \n",
    "    # Prepare input\n",
    "    input_image, target = transform(image, {\"exemplars\": exemplar_tensor})\n",
    "    input_image = input_image.to(device)\n",
    "    input_exemplar = target[\"exemplars\"].to(device)\n",
    "    \n",
    "    # Format text prompt\n",
    "    input_text = text_prompt + \" .\"\n",
    "    \n",
    "    # Run inference\n",
    "    with torch.no_grad():\n",
    "        model_output = model(\n",
    "            input_image.unsqueeze(0),\n",
    "            [input_exemplar],\n",
    "            [torch.tensor([0]).to(device)],\n",
    "            captions=[input_text],\n",
    "        )\n",
    "    \n",
    "    # Extract predictions\n",
    "    logits = model_output[\"pred_logits\"][0].sigmoid()\n",
    "    boxes = model_output[\"pred_boxes\"][0]\n",
    "    \n",
    "    # Filter by confidence threshold\n",
    "    box_mask = logits.max(dim=-1).values > confidence_thresh\n",
    "    filtered_logits = logits[box_mask, :]\n",
    "    filtered_boxes = boxes[box_mask, :]\n",
    "    pred_count = filtered_boxes.shape[0]\n",
    "    \n",
    "    return filtered_boxes, filtered_logits, pred_count\n",
    "\n",
    "def visualize_countgd_results(dataset, image_name, model, transform, confidence_thresh=0.3):\n",
    "    \"\"\"\n",
    "    Run CountGD on a PairTally image and visualize results comparing ground truth vs predictions.\n",
    "    \"\"\"\n",
    "    if not model_ready:\n",
    "        print(\"CountGD model not available\")\n",
    "        return\n",
    "        \n",
    "    try:\n",
    "        # Get image and annotation\n",
    "        img_path = dataset.get_image_path(image_name)\n",
    "        annotation = dataset.get_annotation(image_name)\n",
    "        \n",
    "        if not img_path.exists():\n",
    "            print(f\"ERROR: Image not found: {img_path}\")\n",
    "            return\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        img_array = np.array(image)\n",
    "        \n",
    "        # Get annotation data\n",
    "        positive_points = annotation.get('points', [])\n",
    "        negative_points = annotation.get('negative_points', [])\n",
    "        positive_boxes = annotation.get('box_examples_coordinates', [])\n",
    "        negative_boxes = annotation.get('negative_box_exemples_coordinates', [])\n",
    "        positive_prompt = annotation.get('positive_prompt', 'objects')\n",
    "        negative_prompt = annotation.get('negative_prompt', 'objects')\n",
    "        \n",
    "        # Convert FSC147 format boxes to [x1, y1, x2, y2] format\n",
    "        def convert_boxes(box_coords_list):\n",
    "            boxes = []\n",
    "            for box_coords in box_coords_list[:3]:  # Use first 3 exemplars\n",
    "                if len(box_coords) == 4:\n",
    "                    x_coords = [pt[0] for pt in box_coords]\n",
    "                    y_coords = [pt[1] for pt in box_coords]\n",
    "                    x1, x2 = min(x_coords), max(x_coords)\n",
    "                    y1, y2 = min(y_coords), max(y_coords)\n",
    "                    boxes.append([x1, y1, x2, y2])\n",
    "            return boxes\n",
    "        \n",
    "        pos_exemplar_boxes = convert_boxes(positive_boxes)\n",
    "        neg_exemplar_boxes = convert_boxes(negative_boxes)\n",
    "        \n",
    "        # Get image dimensions for box conversion\n",
    "        img_width, img_height = image.size\n",
    "        \n",
    "        print(f\"Running CountGD on: {image_name}\")\n",
    "        print(f\"Ground Truth - {positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}\")\n",
    "        \n",
    "        # Run inference for positive class\n",
    "        print(f\"\\nInferring positive class ({positive_prompt})...\")\n",
    "        pos_pred_boxes, pos_pred_logits, pos_pred_count = run_countgd_inference(\n",
    "            model, transform, image, pos_exemplar_boxes, positive_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        # Run inference for negative class  \n",
    "        print(f\"\\nInferring negative class ({negative_prompt})...\")\n",
    "        neg_pred_boxes, neg_pred_logits, neg_pred_count = run_countgd_inference(\n",
    "            model, transform, image, neg_exemplar_boxes, negative_prompt, confidence_thresh=confidence_thresh\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nCountGD Predictions - {positive_prompt}: {pos_pred_count}, {negative_prompt}: {neg_pred_count}\")\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        fig.suptitle(f'CountGD Evaluation: {image_name}', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        # 1. Ground Truth with exemplars (no dots, just exemplar boxes)\n",
    "        axes[0].imshow(img_array)\n",
    "        axes[0].set_title(f'Ground Truth + Exemplars\\n{positive_prompt}: {len(positive_points)}, {negative_prompt}: {len(negative_points)}', fontsize=10)\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        # Draw exemplar boxes only (no ground truth points)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[0].add_patch(rect)\n",
    "        \n",
    "        # 2. Positive class predictions\n",
    "        axes[1].imshow(img_array)\n",
    "        axes[1].set_title(f'{positive_prompt} Predictions\\nGT: {len(positive_points)}, Pred: {pos_pred_count}', fontsize=10)\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        # Draw positive exemplars (dashed)\n",
    "        for box in pos_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none', linestyle='--')\n",
    "            axes[1].add_patch(rect)\n",
    "        \n",
    "        # Draw positive predictions (solid boxes)\n",
    "        if len(pos_pred_boxes) > 0 and pos_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            pos_pred_boxes_pixel = pos_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in pos_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width  \n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#0066FF', facecolor='none')\n",
    "                    axes[1].add_patch(rect)\n",
    "        \n",
    "        # 3. Negative class predictions\n",
    "        axes[2].imshow(img_array)\n",
    "        axes[2].set_title(f'{negative_prompt} Predictions\\nGT: {len(negative_points)}, Pred: {neg_pred_count}', fontsize=10)\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        # Draw negative exemplars (dashed)\n",
    "        for box in neg_exemplar_boxes:\n",
    "            x1, y1, x2, y2 = box\n",
    "            rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none', linestyle='--')\n",
    "            axes[2].add_patch(rect)\n",
    "        \n",
    "        # Draw negative predictions (solid boxes)\n",
    "        if len(neg_pred_boxes) > 0 and neg_pred_count > 0:\n",
    "            # Convert from normalized coordinates [0,1] to pixel coordinates\n",
    "            neg_pred_boxes_pixel = neg_pred_boxes.clone().cpu().numpy()\n",
    "            \n",
    "            # CountGD outputs are in format [cx, cy, w, h] normalized\n",
    "            # Convert to [x1, y1, x2, y2] pixel coordinates\n",
    "            for box in neg_pred_boxes_pixel:\n",
    "                cx, cy, w, h = box\n",
    "                # Convert from normalized [cx, cy, w, h] to pixel [x1, y1, x2, y2]\n",
    "                x1 = (cx - w/2) * img_width\n",
    "                y1 = (cy - h/2) * img_height\n",
    "                x2 = (cx + w/2) * img_width\n",
    "                y2 = (cy + h/2) * img_height\n",
    "                \n",
    "                # Clamp to image bounds\n",
    "                x1 = max(0, min(img_width, x1))\n",
    "                y1 = max(0, min(img_height, y1))\n",
    "                x2 = max(0, min(img_width, x2))\n",
    "                y2 = max(0, min(img_height, y2))\n",
    "                \n",
    "                if x2 > x1 and y2 > y1:  # Valid box\n",
    "                    rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='#FF0040', facecolor='none')\n",
    "                    axes[2].add_patch(rect)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate errors\n",
    "        pos_error = abs(pos_pred_count - len(positive_points))\n",
    "        neg_error = abs(neg_pred_count - len(negative_points))\n",
    "        total_error = pos_error + neg_error\n",
    "        \n",
    "        print(f\"\\nEvaluation Results:\")\n",
    "        print(f\"  {positive_prompt} - GT: {len(positive_points)}, Pred: {pos_pred_count}, Error: {pos_error}\")\n",
    "        print(f\"  {negative_prompt} - GT: {len(negative_points)}, Pred: {neg_pred_count}, Error: {neg_error}\")\n",
    "        print(f\"  Total Error: {total_error}\")\n",
    "        print(f\"  Confidence Threshold: {confidence_thresh}\")\n",
    "        \n",
    "        return {\n",
    "            'positive_gt': len(positive_points),\n",
    "            'positive_pred': pos_pred_count,\n",
    "            'positive_error': pos_error,\n",
    "            'negative_gt': len(negative_points),\n",
    "            'negative_pred': neg_pred_count,\n",
    "            'negative_error': neg_error,\n",
    "            'total_error': total_error\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in CountGD evaluation: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "print(\"CountGD inference functions loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo CountGD on sample images\n",
    "def demo_countgd_inference(dataset, model, transform, num_examples=3):\n",
    "    \"\"\"\n",
    "    Demonstrate CountGD inference on sample PairTally images\n",
    "    \"\"\"\n",
    "    if not model_ready:\n",
    "        print(\"CountGD model not available - please set up the model first\")\n",
    "        print(\"\\nTo set up CountGD:\")\n",
    "        print(\"1. cd models/countgd\")\n",
    "        print(\"2. Follow setup instructions in README\")\n",
    "        print(\"3. Download model checkpoint\")\n",
    "        return\n",
    "        \n",
    "    print(\"COUNTGD INFERENCE DEMO\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Demonstrating CountGD on PairTally images...\")\n",
    "    print(\"This shows how counting models perform on fine-grained discrimination tasks\\n\")\n",
    "    \n",
    "    # Get sample images from different subsets\n",
    "    inter_samples = random.sample(list(dataset.inter_annotations.keys()), min(2, len(dataset.inter_annotations)))\n",
    "    intra_samples = random.sample(list(dataset.intra_annotations.keys()), min(2, len(dataset.intra_annotations)))\n",
    "    \n",
    "    all_samples = inter_samples + intra_samples\n",
    "    results = []\n",
    "    \n",
    "    for i, image_name in enumerate(all_samples[:num_examples]):\n",
    "        print(f\"\\nExample {i+1}/{len(all_samples[:num_examples])}: {image_name}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        result = visualize_countgd_results(dataset, image_name, model, transform)\n",
    "        if result:\n",
    "            results.append(result)\n",
    "    \n",
    "    # Summary statistics\n",
    "    if results:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"SUMMARY STATISTICS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        total_pos_error = sum(r['positive_error'] for r in results)\n",
    "        total_neg_error = sum(r['negative_error'] for r in results)\n",
    "        total_error = sum(r['total_error'] for r in results)\n",
    "        avg_error = total_error / len(results)\n",
    "        \n",
    "        print(f\"Images evaluated: {len(results)}\")\n",
    "        print(f\"Total positive class error: {total_pos_error}\")\n",
    "        print(f\"Total negative class error: {total_neg_error}\")\n",
    "        print(f\"Total combined error: {total_error}\")\n",
    "        print(f\"Average error per image: {avg_error:.2f}\")\n",
    "        \n",
    "        print(\"\\nDetailed Results:\")\n",
    "        for i, result in enumerate(results):\n",
    "            print(f\"  Image {i+1}: Pos Error = {result['positive_error']}, Neg Error = {result['negative_error']}, Total = {result['total_error']}\")\n",
    "\n",
    "# Run CountGD demo if model is available\n",
    "if model_ready:\n",
    "    print(\"Running CountGD inference demonstration...\")\n",
    "    demo_countgd_inference(dataset, countgd_model, countgd_transform, num_examples=3)\n",
    "else:\n",
    "    print(\"CountGD model not available for inference demo\")\n",
    "    print(\"\\nTo enable CountGD inference:\")\n",
    "    print(\"1. Set up CountGD model in models/countgd/\")\n",
    "    print(\"2. Download required checkpoint files\")\n",
    "    print(\"3. Install CountGD dependencies\")\n",
    "    print(\"4. Re-run this notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Replicate Results from Paper\n",
    "\n",
    "This section allows you to replicate the GeCo results from the paper.\n",
    "\n",
    "### Instructions:\n",
    "1. **Set up the environment and model:**\n",
    "   ```\n",
    "   models/geco/README.md\n",
    "   ```\n",
    "2. **Run single-class evaluation:**\n",
    "   ```bash\n",
    "   cd models/geco\n",
    "   bash run_count_one_class.sh\n",
    "   ```\n",
    "\n",
    "3. **Run dual-class evaluation:**\n",
    "   ```bash\n",
    "   cd models/geco\n",
    "   bash run_count_both_classes.sh\n",
    "   ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========= CONFIG (edit to your paths/names) =========\n",
    "RESULTS_DIR = 'results'\n",
    "DATASET     = 'pairtally_dataset'\n",
    "MODEL_NAME  = 'CountGD'\n",
    "OUTPUT_DIR  = 'countgd_analysis'\n",
    "SAVE_FIGS   = True\n",
    "\n",
    "QUAL_PATH   = f'{RESULTS_DIR}/{MODEL_NAME}-qualitative/{DATASET}/complete_qualitative_data.json'\n",
    "# =====================================================\n",
    "\n",
    "import json, os\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.precision = 2\n",
    "\n",
    "# Resolve a combined path now that 'os' is imported\n",
    "_COMB_CANDIDATES = [\n",
    "    f'{RESULTS_DIR}/{MODEL_NAME}-count-both-classes-qualitative/{DATASET}/{MODEL_NAME}-Combined_detailed_results.json',\n",
    "    f'{RESULTS_DIR}/{MODEL_NAME}-qualitative-combined/{DATASET}/combined_inference_data.json',\n",
    "    f'{RESULTS_DIR}/{MODEL_NAME}-qualitative-combined/{DATASET}/{MODEL_NAME}-Combined_detailed_results.json',\n",
    "]\n",
    "COMB_PATH = next((p for p in _COMB_CANDIDATES if os.path.exists(p)), None)\n",
    "\n",
    "# ---------------- utils ----------------\n",
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def basename(p): return os.path.basename(p)\n",
    "\n",
    "def parse_filename_meta(filename):\n",
    "    \"\"\"\n",
    "    Parse only metadata (test_type, super_category) from filename like:\n",
    "      ..._{INTRA|INTER}_{super_category}_...\n",
    "    We DO NOT trust counts from filename anymore; GT in JSON is authoritative.\n",
    "    \"\"\"\n",
    "    stem = os.path.splitext(basename(filename))[0]\n",
    "    parts = stem.split('_')\n",
    "    test_type, super_cat = None, None\n",
    "    for i, p in enumerate(parts):\n",
    "        if p in ('INTRA','INTER'):\n",
    "            test_type = p\n",
    "            super_cat = parts[i+1] if i+1 < len(parts) else None\n",
    "            break\n",
    "    return {'test_type': test_type, 'super_category': super_cat}\n",
    "\n",
    "def schema_report(obj, name):\n",
    "    print(f'\\n=== Schema Report: {name} ===')\n",
    "    if isinstance(obj, dict):\n",
    "        print('Top-level keys:', list(obj.keys())[:20])\n",
    "        for k, v in obj.items():\n",
    "            if isinstance(v, list) and v and isinstance(v[0], dict):\n",
    "                print(f'Array-of-dicts key: {k} (len {len(v)})')\n",
    "                print('Sample item keys:', list(v[0].keys())[:30])\n",
    "                break\n",
    "    elif isinstance(obj, list) and obj and isinstance(obj[0], dict):\n",
    "        print('Top-level list len:', len(obj))\n",
    "        print('Sample item keys:', list(obj[0].keys())[:30])\n",
    "\n",
    "# ---------------- loaders ----------------\n",
    "def extract_single_class_items(data):\n",
    "    \"\"\"\n",
    "    Extract rows with: image_name, pred_count, class_type,\n",
    "    gt_pos_count, gt_neg_count, gt_total_count (when present).\n",
    "    Also maps per-class `gt_count` into pos/neg depending on class_type.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "\n",
    "    def push(it, cls_hint=None):\n",
    "        image_name = it.get('image_name') or it.get('filename') or it.get('image') or it.get('name')\n",
    "        pred_count = it.get('pred_count') or it.get('predicted_count') or it.get('count') or it.get('prediction')\n",
    "        cls        = (it.get('class_type') or it.get('class') or it.get('label') or cls_hint or '').lower()\n",
    "        if cls not in ('positive','negative'):\n",
    "            cls = 'positive'\n",
    "\n",
    "        # Prefer explicit GT fields if present\n",
    "        gt_pos = it.get('gt_pos_count')\n",
    "        gt_neg = it.get('gt_neg_count')\n",
    "        gt_tot = it.get('gt_total_count')\n",
    "\n",
    "        # Some dumps only have gt_count for the queried class\n",
    "        gt_one = it.get('gt_count')\n",
    "        if gt_one is not None:\n",
    "            if cls == 'positive' and gt_pos is None:\n",
    "                gt_pos = int(gt_one)\n",
    "            if cls == 'negative' and gt_neg is None:\n",
    "                gt_neg = int(gt_one)\n",
    "\n",
    "        if image_name is not None and pred_count is not None:\n",
    "            out.append({\n",
    "                'image_name': image_name,\n",
    "                'pred_count': int(pred_count),\n",
    "                'class_type': cls,\n",
    "                'gt_pos_count': (int(gt_pos) if gt_pos is not None else None),\n",
    "                'gt_neg_count': (int(gt_neg) if gt_neg is not None else None),\n",
    "                'gt_total_count': (int(gt_tot) if gt_tot is not None else None),\n",
    "            })\n",
    "\n",
    "    # CountGD qualitative usually: {\"class_results\": {\"positive\": {\"images\":[...]}, \"negative\": {\"images\":[...]}}}\n",
    "    if isinstance(data, dict) and 'class_results' in data:\n",
    "        cr = data['class_results']\n",
    "        for cls in ('positive','negative'):\n",
    "            for it in cr.get(cls, {}).get('images', []):\n",
    "                if isinstance(it, dict):\n",
    "                    push(it, cls_hint=cls)\n",
    "    elif isinstance(data, list):\n",
    "        for it in data:\n",
    "            if isinstance(it, dict):\n",
    "                push(it)\n",
    "    else:\n",
    "        for v in (data.values() if isinstance(data, dict) else []):\n",
    "            if isinstance(v, list):\n",
    "                for it in v:\n",
    "                    if isinstance(it, dict):\n",
    "                        push(it)\n",
    "    return out\n",
    "\n",
    "def extract_combined_items(data):\n",
    "    \"\"\"\n",
    "    Return dict keyed by BASENAME(filename) -> {'pred_total': int|None, 'gt_total': int|None}\n",
    "    Supports common CountGD combined shapes and your `combined_gt_count`.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    if not data:\n",
    "        return out\n",
    "\n",
    "    def put(fn, pred=None, gt=None):\n",
    "        if not fn:\n",
    "            return\n",
    "        base = basename(fn)\n",
    "        rec = out.setdefault(base, {'pred_total': None, 'gt_total': None})\n",
    "        if pred is not None:\n",
    "            rec['pred_total'] = int(pred)\n",
    "        if gt is not None:\n",
    "            rec['gt_total'] = int(gt)\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        # A) image_results mapping\n",
    "        if isinstance(data.get('image_results'), dict):\n",
    "            for fn, obj in data['image_results'].items():\n",
    "                if isinstance(obj, dict):\n",
    "                    pc = (\n",
    "                        obj.get('combined_predicted_count') or\n",
    "                        obj.get('pred_total_count') or\n",
    "                        obj.get('predicted_count') or\n",
    "                        obj.get('pred_count') or\n",
    "                        obj.get('count') or\n",
    "                        obj.get('prediction')\n",
    "                    )\n",
    "                    gt = (\n",
    "                        obj.get('combined_gt_count') or     # <-- your key\n",
    "                        obj.get('gt_total_count') or\n",
    "                        obj.get('total_gt') or\n",
    "                        obj.get('gt_count_total')\n",
    "                    )\n",
    "                    put(fn, pc, gt)\n",
    "\n",
    "        # B) results -> images list\n",
    "        res = data.get('results')\n",
    "        if isinstance(res, dict) and isinstance(res.get('images'), list):\n",
    "            for it in res['images']:\n",
    "                if not isinstance(it, dict): continue\n",
    "                fn = it.get('image_name') or it.get('filename') or it.get('image') or it.get('name')\n",
    "                pc = (\n",
    "                    it.get('pred_total_count') or\n",
    "                    it.get('predicted_count') or\n",
    "                    it.get('pred_count') or\n",
    "                    it.get('count') or\n",
    "                    it.get('prediction')\n",
    "                )\n",
    "                gt = (\n",
    "                    it.get('combined_gt_count') or         # handle here too, just in case\n",
    "                    it.get('gt_total_count') or\n",
    "                    it.get('total_gt') or\n",
    "                    it.get('gt_count_total')\n",
    "                )\n",
    "                put(fn, pc, gt)\n",
    "\n",
    "        # C) any other top-level lists\n",
    "        for v in data.values():\n",
    "            if isinstance(v, list):\n",
    "                for it in v:\n",
    "                    if isinstance(it, dict):\n",
    "                        fn = it.get('image_name') or it.get('filename') or it.get('image') or it.get('name')\n",
    "                        pc = (\n",
    "                            it.get('pred_total_count') or\n",
    "                            it.get('predicted_count') or\n",
    "                            it.get('pred_count') or\n",
    "                            it.get('count') or\n",
    "                            it.get('prediction')\n",
    "                        )\n",
    "                        gt = (\n",
    "                            it.get('combined_gt_count') or\n",
    "                            it.get('gt_total_count') or\n",
    "                            it.get('total_gt') or\n",
    "                            it.get('gt_count_total')\n",
    "                        )\n",
    "                        put(fn, pc, gt)\n",
    "\n",
    "    elif isinstance(data, list):\n",
    "        for it in data:\n",
    "            if not isinstance(it, dict): continue\n",
    "            fn = it.get('image_name') or it.get('filename') or it.get('image') or it.get('name')\n",
    "            pc = (\n",
    "                it.get('pred_total_count') or\n",
    "                it.get('predicted_count') or\n",
    "                it.get('pred_count') or\n",
    "                it.get('count') or\n",
    "                it.get('prediction')\n",
    "            )\n",
    "            gt = (\n",
    "                it.get('combined_gt_count') or\n",
    "                it.get('gt_total_count') or\n",
    "                it.get('total_gt') or\n",
    "                it.get('gt_count_total')\n",
    "            )\n",
    "            put(fn, pc, gt)\n",
    "\n",
    "    return out\n",
    "\n",
    "def load_predictions_countgd(qual_path, comb_path):\n",
    "    qd = read_json(qual_path)\n",
    "    schema_report(qd, 'QUALITATIVE JSON')\n",
    "    cd = read_json(comb_path) if comb_path and os.path.exists(comb_path) else None\n",
    "    if cd is not None:\n",
    "        schema_report(cd, 'COMBINED JSON')\n",
    "\n",
    "    single_items = extract_single_class_items(qd)\n",
    "    combined_map = extract_combined_items(cd)\n",
    "\n",
    "    print(f\"\\nFound {len(single_items)} single-class items; combined predictions: {len(combined_map)}\")\n",
    "\n",
    "    image_predictions = {}\n",
    "    kept, skipped = 0, 0\n",
    "\n",
    "    for it in single_items:\n",
    "        fname = it['image_name']\n",
    "        base  = basename(fname)\n",
    "        cls   = it['class_type']\n",
    "        key   = (base, cls)\n",
    "\n",
    "        meta = parse_filename_meta(base)  # test_type / super_category only\n",
    "\n",
    "        # Prefer JSON GT; compute total if missing but pos+neg exist\n",
    "        gt_pos = it['gt_pos_count']\n",
    "        gt_neg = it['gt_neg_count']\n",
    "        gt_tot = it['gt_total_count']\n",
    "        if gt_tot is None and (gt_pos is not None and gt_neg is not None):\n",
    "            gt_tot = gt_pos + gt_neg\n",
    "\n",
    "        # If still missing total, fall back to combined GT if available\n",
    "        if gt_tot is None and base in combined_map and combined_map[base]['gt_total'] is not None:\n",
    "            gt_tot = combined_map[base]['gt_total']\n",
    "\n",
    "        # Choose a_true based on class\n",
    "        a_true = gt_neg if cls == 'negative' else gt_pos\n",
    "\n",
    "        # Strict: require a and total; otherwise skip\n",
    "        if a_true is None or gt_tot is None:\n",
    "            skipped += 1\n",
    "            continue\n",
    "\n",
    "        b_true = gt_tot - a_true\n",
    "\n",
    "        image_predictions[key] = {\n",
    "            'filename': base,\n",
    "            'class_type': cls,\n",
    "            'test_type': meta['test_type'],\n",
    "            'super_category': meta['super_category'],\n",
    "            'f_A': int(it['pred_count']),\n",
    "            'a': int(a_true),\n",
    "            'b': int(b_true),\n",
    "            'a_plus_b': int(gt_tot),\n",
    "            'f_A_plus_B': (\n",
    "                combined_map[base]['pred_total']\n",
    "                if base in combined_map and combined_map[base]['pred_total'] is not None\n",
    "                else np.nan\n",
    "            ),\n",
    "        }\n",
    "        kept += 1\n",
    "\n",
    "    print(f\"Kept {kept} rows; skipped {skipped} (missing GT).\")\n",
    "    return image_predictions\n",
    "\n",
    "# ---------------- metrics & summaries ----------------\n",
    "def per_image_metrics(image_predictions):\n",
    "    rows = []\n",
    "    for (_, _), d in image_predictions.items():\n",
    "        f_A        = d['f_A']\n",
    "        a          = d['a']\n",
    "        a_plus_b   = d['a_plus_b']\n",
    "        f_A_plus_B = d.get('f_A_plus_B', np.nan)\n",
    "\n",
    "        rows.append({\n",
    "            'filename': d['filename'],\n",
    "            'class_type': d['class_type'],\n",
    "            'test_type': d['test_type'],\n",
    "            'super_category': d['super_category'],\n",
    "            'f_A': f_A,\n",
    "            'f_A_plus_B': f_A_plus_B,\n",
    "            'a': a,\n",
    "            'b': d['b'],\n",
    "            'a_plus_b': a_plus_b,\n",
    "            '|f(A)-a|': abs(f_A - a),\n",
    "            '|f(A+B)-(a+b)|': abs((f_A_plus_B if not np.isnan(f_A_plus_B) else f_A) - a_plus_b),\n",
    "            '|f(A)-(a+b)|': abs(f_A - a_plus_b),\n",
    "            '|f(A)-f(A+B)|': abs(f_A - (f_A_plus_B if not np.isnan(f_A_plus_B) else f_A)),\n",
    "            'f(A)>a': f_A > a,\n",
    "            '|f(A)-a|>|f(A)-(a+b)|': abs(f_A - a) > abs(f_A - a_plus_b),\n",
    "        })\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def summarize_metrics(df):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame([{\n",
    "            'total_predictions': 0,\n",
    "            'mean |f(A)-a|': np.nan,\n",
    "            'mean |f(A+B)-(a+b)|': np.nan,\n",
    "            'mean |f(A)-(a+b)|': np.nan,\n",
    "            'mean |f(A)-f(A+B)|': np.nan,\n",
    "            'f(A)>a %': np.nan,\n",
    "            '|f(A)-a|>|f(A)-(a+b)| %': np.nan\n",
    "        }])\n",
    "    total = len(df)\n",
    "    return pd.DataFrame([{\n",
    "        'total_predictions': total,\n",
    "        'mean |f(A)-a|': df['|f(A)-a|'].mean(),\n",
    "        'mean |f(A+B)-(a+b)|': df['|f(A+B)-(a+b)|'].mean(),\n",
    "        'mean |f(A)-(a+b)|': df['|f(A)-(a+b)|'].mean(),\n",
    "        'mean |f(A)-f(A+B)|': df['|f(A)-f(A+B)|'].mean(),\n",
    "        'f(A)>a %': 100.0 * df['f(A)>a'].mean(),\n",
    "        '|f(A)-a|>|f(A)-(a+b)| %': 100.0 * df['|f(A)-a|>|f(A)-(a+b)|'].mean(),\n",
    "    }])\n",
    "\n",
    "def summarize_by(df, group_cols):\n",
    "    if df.empty: return pd.DataFrame()\n",
    "    parts = []\n",
    "    for keys, g in df.groupby(group_cols):\n",
    "        s = summarize_metrics(g)\n",
    "        row = {col: val for col, val in zip(group_cols, keys if isinstance(keys, tuple) else (keys,))}\n",
    "        parts.append(pd.concat([pd.DataFrame([row]), s], axis=1))\n",
    "    return pd.concat(parts, ignore_index=True)\n",
    "\n",
    "def plot_overall_bars(summary_df, title_prefix=MODEL_NAME):\n",
    "    if summary_df.empty or summary_df['total_predictions'].iloc[0] == 0:\n",
    "        print(\"No data to plot.\"); return\n",
    "\n",
    "    mets = ['mean |f(A)-a|', 'mean |f(A+B)-(a+b)|', 'mean |f(A)-(a+b)|', 'mean |f(A)-f(A+B)|']\n",
    "    vals = summary_df.loc[0, mets].astype(float).values\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.bar(mets, vals)\n",
    "    plt.ylabel('Mean Absolute Error'); plt.title(f'{title_prefix}: Fine-Grained Counting Metrics (Means)')\n",
    "    plt.xticks(rotation=20, ha='right'); plt.grid(True, axis='y', alpha=0.3); plt.tight_layout()\n",
    "    if SAVE_FIGS:\n",
    "        os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_fine_grained_metrics.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_fine_grained_metrics.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    mets2 = ['f(A)>a %', '|f(A)-a|>|f(A)-(a+b)| %']\n",
    "    vals2 = summary_df.loc[0, mets2].astype(float).values\n",
    "\n",
    "    plt.figure(figsize=(7,5))\n",
    "    plt.bar(mets2, vals2)\n",
    "    plt.ylabel('Percentage'); plt.title(f'{title_prefix}: Overcounting Indicators')\n",
    "    plt.xticks(rotation=10, ha='right'); plt.grid(True, axis='y', alpha=0.3); plt.tight_layout()\n",
    "    if SAVE_FIGS:\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_overcounting_percentages.png'), dpi=300, bbox_inches='tight')\n",
    "        plt.savefig(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_overcounting_percentages.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# ---------------- run ----------------\n",
    "display(Markdown(f\"# {MODEL_NAME} Overcounting Experiment\"))\n",
    "print(\"Hypothesis: the single-class prediction f(A) is closer to total count (a+b) than to a.\\n\")\n",
    "print(f\"Single-class data: {QUAL_PATH}\")\n",
    "print(\"Combined data    :\", (COMB_PATH if COMB_PATH else \"(not found — proceeding without it)\"))\n",
    "\n",
    "if not os.path.exists(QUAL_PATH):\n",
    "    raise FileNotFoundError(f\"Qualitative data file not found: {QUAL_PATH}\")\n",
    "\n",
    "preds_dict = load_predictions_countgd(QUAL_PATH, COMB_PATH)\n",
    "df_img = per_image_metrics(preds_dict)\n",
    "\n",
    "print(f\"\\nParsed {len(df_img):,} per-(image,class) rows.\\n\")\n",
    "if df_img.empty:\n",
    "    display(Markdown(\"> **No rows parsed** — check that your qualitative JSON includes `gt_pos_count/gt_neg_count/gt_total_count` or at least per-row `gt_count` for the queried class.\"))\n",
    "else:\n",
    "    display(Markdown(\"### Sample rows\"))\n",
    "    display(df_img.head(10))\n",
    "\n",
    "summary = summarize_metrics(df_img)\n",
    "summary.insert(0, 'Model', MODEL_NAME)\n",
    "display(Markdown(\"## Overall Summary\"))\n",
    "display(summary.style.format({\n",
    "    'mean |f(A)-a|': '{:.2f}',\n",
    "    'mean |f(A+B)-(a+b)|': '{:.2f}',\n",
    "    'mean |f(A)-(a+b)|': '{:.2f}',\n",
    "    'mean |f(A)-f(A+B)|': '{:.2f}',\n",
    "    'f(A)>a %': '{:.1f}',\n",
    "    '|f(A)-a|>|f(A)-(a+b)| %': '{:.1f}',\n",
    "    'total_predictions': '{:,.0f}'\n",
    "}))\n",
    "\n",
    "by_test = summarize_by(df_img, ['test_type'])\n",
    "if not by_test.empty:\n",
    "    by_test.insert(0, 'Model', MODEL_NAME)\n",
    "    display(Markdown(\"## By Test Type\"))\n",
    "    display(by_test.style.format({\n",
    "        'mean |f(A)-a|': '{:.2f}',\n",
    "        'mean |f(A+B)-(a+b)|': '{:.2f}',\n",
    "        'mean |f(A)-(a+b)|': '{:.2f}',\n",
    "        'mean |f(A)-f(A+B)|': '{:.2f}',\n",
    "        'f(A)>a %': '{:.1f}',\n",
    "        '|f(A)-a|>|f(A)-(a+b)| %': '{:.1f}',\n",
    "        'total_predictions': '{:,.0f}'\n",
    "    }))\n",
    "\n",
    "by_super = summarize_by(df_img, ['super_category'])\n",
    "if not by_super.empty:\n",
    "    by_super.insert(0, 'Model', MODEL_NAME)\n",
    "    display(Markdown(\"## By Super Category\"))\n",
    "    display(by_super.style.format({\n",
    "        'mean |f(A)-a|': '{:.2f}',\n",
    "        'mean |f(A+B)-(a+b)|': '{:.2f}',\n",
    "        'mean |f(A)-(a+b)|': '{:.2f}',\n",
    "        'mean |f(A)-f(A+B)|': '{:.2f}',\n",
    "        'f(A)>a %': '{:.1f}',\n",
    "        '|f(A)-a|>|f(A)-(a+b)| %': '{:.1f}',\n",
    "        'total_predictions': '{:,.0f}'\n",
    "    }))\n",
    "\n",
    "plot_overall_bars(summary, title_prefix=MODEL_NAME)\n",
    "\n",
    "if SAVE_FIGS:\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "    df_img.to_csv(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_per_image_metrics_{DATASET}.csv'), index=False)\n",
    "    summary.to_csv(os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_overcounting_summary_{DATASET}.csv'), index=False)\n",
    "    print(f\"Saved tables:\\n  {os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_per_image_metrics_{DATASET}.csv')}\\n  {os.path.join(OUTPUT_DIR, f'{MODEL_NAME.lower()}_overcounting_summary_{DATASET}.csv')}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "countgd_notebook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
